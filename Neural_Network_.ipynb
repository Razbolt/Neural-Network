{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Razbolt/Neural-Network/blob/greg/Neural_Network_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGR6imnPESL"
      },
      "source": [
        "#1.1 Import the libraries we need and load the dataset using Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j7joMCD_8U_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc1NylZkLXAz",
        "outputId": "a8693de8-48d1-4701-d446-0f3abf0e5555"
      },
      "outputs": [],
      "source": [
        "# Use keras to import MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6uJCuvDsoP2"
      },
      "source": [
        "##1.2 Data inspection and preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4kGn7-Ps25y"
      },
      "source": [
        "Before procesing, we need to check the MNIST Data.\n",
        "First we plot one sample image to see what our input data look like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "agbg6KpItreT",
        "outputId": "01929e05-179f-45c0-fbcd-70cb54de58a2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number shown is: 3\n"
          ]
        }
      ],
      "source": [
        "# Plot a sample image\n",
        "sample = 7\n",
        "image = X_train[sample]\n",
        "# plot the sample\n",
        "fig = plt.figure\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "print(\"The number shown on image is:\", y_train[sample])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR2Psgt-uJ8d"
      },
      "source": [
        "Then we print the size of the arrays we have created just to make sure they are as expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ1--VdWCEzT",
        "outputId": "57913413-7f5e-487a-9ebb-3f1fa2abb060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of X_train (60000, 28, 28)\n",
            "shape of X_test (10000, 28, 28)\n",
            "shape of y_train (60000,)\n",
            "shape of y_test (10000,)\n"
          ]
        }
      ],
      "source": [
        "print (\"shape of X_train {}\".format(X_train.shape))\n",
        "print (\"shape of X_test {}\".format(X_test.shape))\n",
        "print (\"shape of y_train {}\".format(y_train.shape))\n",
        "print (\"shape of y_test {}\".format(y_test.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J3kJFKLuWZz"
      },
      "source": [
        "Finally we print one data sample to see what it is like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRu5VJIiuc7j",
        "outputId": "0f485ee9-bbdc-4f7c-c4aa-4a689a74d7b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,  43,\n",
              "        105, 255, 253, 253, 253, 253, 253, 174,   6,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  43, 139, 224, 226,\n",
              "        252, 253, 252, 252, 252, 252, 252, 252, 158,  14,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 178, 252, 252, 252,\n",
              "        252, 253, 252, 252, 252, 252, 252, 252, 252,  59,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 109, 252, 252, 230,\n",
              "        132, 133, 132, 132, 189, 252, 252, 252, 252,  59,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,  29,  29,  24,\n",
              "          0,   0,   0,   0,  14, 226, 252, 252, 172,   7,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,  85, 243, 252, 252, 144,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  88, 189, 252, 252, 252,  14,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  91, 212, 247, 252, 252, 252, 204,   9,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 125, 193, 193,\n",
              "        193, 253, 252, 252, 252, 238, 102,  28,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  45, 222, 252, 252, 252,\n",
              "        252, 253, 252, 252, 252, 177,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  45, 223, 253, 253, 253,\n",
              "        253, 255, 253, 253, 253, 253,  74,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 123,  52,  44,\n",
              "         44,  44,  44, 143, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  15, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  86, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   5,  75,   9,   0,   0,   0,   0,\n",
              "          0,   0,  98, 242, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,  61, 183, 252,  29,   0,   0,   0,   0,\n",
              "         18,  92, 239, 252, 252, 243,  65,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0, 208, 252, 252, 147, 134, 134, 134, 134,\n",
              "        203, 253, 252, 252, 188,  83,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0, 208, 252, 252, 252, 252, 252, 252, 252,\n",
              "        252, 253, 230, 153,   8,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,  49, 157, 252, 252, 252, 252, 252, 217,\n",
              "        207, 146,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   7, 103, 235, 252, 172, 103,  24,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDoduNmWvn2b"
      },
      "source": [
        "We can see that our input is a 28x28 array of integers with values in the range 0 to 255. These correspond to the input image size which is 28x28 pixels and the values represent the colour intensity of each pixel in the image in the greyscale range. 0 is black, 255 is white and the values in between are shades of grey. This can also be confirmed by the image that we have plotted above.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeMY2Oi-y2LS"
      },
      "source": [
        "In order to feed the data into our NN we need to \"flatten\" the 28x28 array into one dimension array containing all 784 elements and preserving the total number of input samples. We print the resulting size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fHFqMAQBRrf",
        "outputId": "ca8af697-9941-4aa7-e283-7ca364b8b6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of X_train_flat (60000, 784)\n",
            "shape of X_test_flat (10000, 784)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  38,  43, 105, 255, 253,\n",
              "       253, 253, 253, 253, 174,   6,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  43, 139, 224, 226, 252,\n",
              "       253, 252, 252, 252, 252, 252, 252, 158,  14,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 178, 252, 252,\n",
              "       252, 252, 253, 252, 252, 252, 252, 252, 252, 252,  59,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 109,\n",
              "       252, 252, 230, 132, 133, 132, 132, 189, 252, 252, 252, 252,  59,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   4,  29,  29,  24,   0,   0,   0,   0,  14, 226, 252, 252,\n",
              "       172,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  85, 243,\n",
              "       252, 252, 144,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  88,\n",
              "       189, 252, 252, 252,  14,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  91,\n",
              "       212, 247, 252, 252, 252, 204,   9,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 125, 193, 193,\n",
              "       193, 253, 252, 252, 252, 238, 102,  28,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  45, 222, 252,\n",
              "       252, 252, 252, 253, 252, 252, 252, 177,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  45,\n",
              "       223, 253, 253, 253, 253, 255, 253, 253, 253, 253,  74,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,  31, 123,  52,  44,  44,  44,  44, 143, 252, 252,  74,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  15, 252,\n",
              "       252,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        86, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   5,  75,   9,   0,   0,   0,   0,   0,\n",
              "         0,  98, 242, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,  61, 183, 252,  29,   0,   0,   0,\n",
              "         0,  18,  92, 239, 252, 252, 243,  65,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0, 208, 252, 252, 147, 134,\n",
              "       134, 134, 134, 203, 253, 252, 252, 188,  83,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 208, 252, 252,\n",
              "       252, 252, 252, 252, 252, 252, 253, 230, 153,   8,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  49,\n",
              "       157, 252, 252, 252, 252, 252, 217, 207, 146,  45,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   7, 103, 235, 252, 172, 103,  24,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=uint8)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "X_test_flat = X_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "print (\"shape of X_train_flat {}\".format(X_train_flat.shape))\n",
        "print (\"shape of X_test_flat {}\".format(X_test_flat.shape))\n",
        "X_train_flat[7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxCpjwqG1OTe"
      },
      "source": [
        "Since all our features are integer values that range from 0 to 255 it is not absolutely necessary to standardize our input data. However we will perform a simple scaling of the data by dividing all values by the max value 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcReBNIP5Cjs",
        "outputId": "fb7fd9ef-701a-4c7f-92c0-fb76cf424f35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.14901961, 0.16862745, 0.41176471, 1.        ,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.68235294, 0.02352941, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.16862745, 0.54509804, 0.87843137,\n",
              "       0.88627451, 0.98823529, 0.99215686, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.98823529, 0.98823529, 0.98823529, 0.61960784,\n",
              "       0.05490196, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.69803922, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.99215686, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.98823529, 0.98823529, 0.23137255, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.42745098, 0.98823529,\n",
              "       0.98823529, 0.90196078, 0.51764706, 0.52156863, 0.51764706,\n",
              "       0.51764706, 0.74117647, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.23137255, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.01568627, 0.11372549, 0.11372549, 0.09411765,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
              "       0.88627451, 0.98823529, 0.98823529, 0.6745098 , 0.02745098,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.33333333, 0.95294118, 0.98823529,\n",
              "       0.98823529, 0.56470588, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.34509804,\n",
              "       0.74117647, 0.98823529, 0.98823529, 0.98823529, 0.05490196,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.35686275, 0.83137255, 0.96862745, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.8       , 0.03529412, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.1254902 , 0.49019608,\n",
              "       0.75686275, 0.75686275, 0.75686275, 0.99215686, 0.98823529,\n",
              "       0.98823529, 0.98823529, 0.93333333, 0.4       , 0.10980392,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.17647059, 0.87058824, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.99215686, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.69411765, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.17647059, 0.8745098 ,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 1.        ,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.29019608,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.12156863, 0.48235294, 0.20392157,\n",
              "       0.17254902, 0.17254902, 0.17254902, 0.17254902, 0.56078431,\n",
              "       0.98823529, 0.98823529, 0.29019608, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.05882353, 0.98823529, 0.98823529,\n",
              "       0.29019608, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.3372549 , 0.98823529, 0.98823529, 0.29019608, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.01960784, 0.29411765,\n",
              "       0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.38431373, 0.94901961, 0.98823529,\n",
              "       0.98823529, 0.29019608, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.23921569, 0.71764706, 0.98823529, 0.11372549, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.07058824, 0.36078431,\n",
              "       0.9372549 , 0.98823529, 0.98823529, 0.95294118, 0.25490196,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.81568627, 0.98823529,\n",
              "       0.98823529, 0.57647059, 0.5254902 , 0.5254902 , 0.5254902 ,\n",
              "       0.5254902 , 0.79607843, 0.99215686, 0.98823529, 0.98823529,\n",
              "       0.7372549 , 0.3254902 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.81568627, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.99215686, 0.90196078, 0.6       , 0.03137255, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.19215686,\n",
              "       0.61568627, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.85098039, 0.81176471, 0.57254902, 0.17647059,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.02745098, 0.40392157,\n",
              "       0.92156863, 0.98823529, 0.6745098 , 0.40392157, 0.09411765,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_scaled = X_train_flat/255\n",
        "X_train_scaled[7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFbxcwcENmQk"
      },
      "source": [
        "#2.1 Build Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uqi_S-ZPhn9"
      },
      "outputs": [],
      "source": [
        "def init_params():\n",
        "\n",
        "  #input layer's weigths\n",
        "  W1 = np.random.uniform(-0.5, 0.5, (16,784))\n",
        "  b1 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Hidden layer's weight\n",
        "  W2 = np.random.uniform(-0.5,0.5, (16,16))\n",
        "  b2 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "  # Second Hidden layer' weight\n",
        "  W3 = np.random.uniform(-0.5,0.5 ,(10,16))  # Weights are still to big\n",
        "  b3 = np.zeros((10,1))\n",
        "\n",
        "\n",
        "  return W1,b1,W2,b2,W3,b3\n",
        "\n",
        "\n",
        "\n",
        "W1,b1,W2,b2,W3,b3 = init_params()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRw2jsVWH0I4"
      },
      "source": [
        "#1.2 Build Activation functions and Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBsAF3drU98m",
        "outputId": "338feec9-0fb4-468a-b7bd-2c24981b3517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.00679114 0.00527258 0.0658074  ... 0.00207419 0.00600942 0.03091759]\n",
            " [0.01790551 0.28869534 0.05381954 ... 0.18655483 0.14435255 0.18302996]\n",
            " [0.78716232 0.30405957 0.36980117 ... 0.50645333 0.02556571 0.22413396]\n",
            " ...\n",
            " [0.0109231  0.03390429 0.05191376 ... 0.04565613 0.09377314 0.06274774]\n",
            " [0.00242549 0.00778729 0.02485256 ... 0.00833378 0.00270395 0.09929324]\n",
            " [0.02089471 0.00800702 0.0968669  ... 0.00419734 0.00102753 0.06981688]]\n"
          ]
        }
      ],
      "source": [
        "def ReLU(Z):\n",
        "\n",
        "  return np.maximum(0,Z)\n",
        "\n",
        "\n",
        "def softmax(Z): # CHECK IT !!!\n",
        "\n",
        "  # for one examples is (np.exp(Z) / np.sum( np.exp(Z)))\n",
        "\n",
        "    Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
        "    sum_Z_exp = np.sum(Z_exp, axis=0)\n",
        "    softmax_output = Z_exp / sum_Z_exp\n",
        "\n",
        "    return softmax_output\n",
        "\n",
        "def sigmoid(Z):\n",
        "\n",
        "  return 1 / 1+ np.exp(-Z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def forward_prop(X,W1,b1,W2,b2,W3,b3):\n",
        "  #Forward propagation with 3 layered neural network\n",
        "\n",
        "  #Initialize  Z\n",
        "  #Since X is shape as (6000,784) and our W1 is shape as (16,784) in order to dot product we need to take Transpose of X\n",
        "\n",
        "  Z1 = W1.dot(X.T)+ b1\n",
        "  #Activation with ReLU to given linear regression to feed neural network\n",
        "  A1 = ReLU(Z1)\n",
        "\n",
        "  Z2 = W2.dot(A1) + b2\n",
        "\n",
        "\n",
        "  A2 = ReLU(Z2)\n",
        "\n",
        "  Z3 = W3.dot(A2) + b3\n",
        "  #At the end softmax  the output layer to have a probabilistic values\n",
        "  A3 = softmax(Z3)\n",
        "\n",
        "  return A1,A2,A3\n",
        "\n",
        "A1,A2,A3 = forward_prop(X_train,W1,b1,W2,b2,W3,b3)\n",
        "\n",
        "print(A3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFn_mUjHPKab"
      },
      "source": [
        "#1.3 Back-Propagation with One-hot and derivative of activaton functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dhSX3T_HAx9"
      },
      "outputs": [],
      "source": [
        "def one_hot(Y): # CHECK IT !!\n",
        "  num_classes = np.max(Y) + 1\n",
        "\n",
        "  one_hot = np.zeros((Y.size,num_classes))\n",
        "  one_hot[np.arange(Y.size), Y] = 1\n",
        "\n",
        "  return one_hot.T\n",
        "\n",
        "def derivative_ReLU(Z):\n",
        "  #Do we need to check between 0-1 ?\n",
        "  return Z > 0\n",
        "\n",
        "\n",
        "def backward_prop(X, Y, W1, b1, W2, b2, W3, b3, A1, A2, A3): # CHECK IT !!!!\n",
        "\n",
        "  m = Y.size\n",
        "  #One hot encoded to see each labels in matrix as 1\n",
        "  Y = one_hot(Y)\n",
        "\n",
        "  #Derivate of cost function with respect to z3\n",
        "  dZ3 = A3 - Y\n",
        "  dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "  dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
        "\n",
        "  #Derivate for second hidden layer\n",
        "  dA2 = np.dot(W3.T,dZ3)\n",
        "  dZ2 =  dA2  * derivative_ReLU(A2)\n",
        "  dW2 = (1/ m) * dZ2.dot(A1.T)\n",
        "  dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
        "\n",
        "  #Derivate for first hidden layer\n",
        "\n",
        "  dA1 = np.dot(W2.T,dZ2)\n",
        "  dZ1 = dA1 * derivative_ReLU(A1)\n",
        "  dW1 = (1/m) * dZ1.dot(X)\n",
        "  dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
        "\n",
        "  return dW1,dB1,dW2,dB2,dW3,dB3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tYISPvOQRQn"
      },
      "source": [
        "#1.3 Update the Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFbTDfoWQYq-"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "epochs =  1000\n",
        "\n",
        "def update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate):\n",
        "\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "\n",
        "    return W1,b1,W2,b2,W3,b3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhb1TpDrQtKt"
      },
      "source": [
        "#1.4 Try in given epochs time to see the changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-QTIprNdkBP"
      },
      "outputs": [],
      "source": [
        "def accuracy_score(A,Y):\n",
        "    size = Y.size\n",
        "    predict = np.argmax(A,0)\n",
        "\n",
        "    correct = np.sum(predict == Y)\n",
        "\n",
        "    accuracy = correct / size\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "#print(predictions.max)\n",
        "#print(y_train)\n",
        "#print(np.sum(y_train == predictions))\n",
        "#print(y_train.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLlbWaJGN7Pw",
        "outputId": "eecb592c-7f38-4726-8423-b3297eaa559c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy in 0 is 0.6655\n",
            "The accuracy in 20 is 0.7005333333333333\n",
            "The accuracy in 40 is 0.72825\n",
            "The accuracy in 60 is 0.7502833333333333\n",
            "The accuracy in 80 is 0.7692\n",
            "The accuracy in 100 is 0.7851666666666667\n",
            "The accuracy in 120 is 0.7992333333333334\n",
            "The accuracy in 140 is 0.81045\n",
            "The accuracy in 160 is 0.8194833333333333\n",
            "The accuracy in 180 is 0.8268333333333333\n",
            "The accuracy in 200 is 0.8329166666666666\n",
            "The accuracy in 220 is 0.8385333333333334\n",
            "The accuracy in 240 is 0.843\n",
            "The accuracy in 260 is 0.8265666666666667\n",
            "The accuracy in 280 is 0.8520166666666666\n",
            "The accuracy in 300 is 0.8557333333333333\n",
            "The accuracy in 320 is 0.8584833333333334\n",
            "The accuracy in 340 is 0.86105\n",
            "The accuracy in 360 is 0.86335\n",
            "The accuracy in 380 is 0.8658833333333333\n",
            "The accuracy in 400 is 0.86815\n",
            "The accuracy in 420 is 0.8708166666666667\n",
            "The accuracy in 440 is 0.8726833333333334\n",
            "The accuracy in 460 is 0.87495\n",
            "The accuracy in 480 is 0.87665\n",
            "The accuracy in 500 is 0.8784666666666666\n",
            "The accuracy in 520 is 0.8802833333333333\n",
            "The accuracy in 540 is 0.8818\n",
            "The accuracy in 560 is 0.8833666666666666\n",
            "The accuracy in 580 is 0.8850166666666667\n",
            "The accuracy in 600 is 0.8861333333333333\n",
            "The accuracy in 620 is 0.8873\n",
            "The accuracy in 640 is 0.8884166666666666\n",
            "The accuracy in 660 is 0.8898166666666667\n",
            "The accuracy in 680 is 0.8907666666666667\n",
            "The accuracy in 700 is 0.89175\n",
            "The accuracy in 720 is 0.8929666666666667\n",
            "The accuracy in 740 is 0.89385\n",
            "The accuracy in 760 is 0.8947166666666667\n",
            "The accuracy in 780 is 0.8958833333333334\n",
            "The accuracy in 800 is 0.8969666666666667\n",
            "The accuracy in 820 is 0.8978\n",
            "The accuracy in 840 is 0.8985333333333333\n",
            "The accuracy in 860 is 0.8991833333333333\n",
            "The accuracy in 880 is 0.9001333333333333\n",
            "The accuracy in 900 is 0.9009833333333334\n",
            "The accuracy in 920 is 0.90185\n",
            "The accuracy in 940 is 0.9025333333333333\n",
            "The accuracy in 960 is 0.9030666666666667\n",
            "The accuracy in 980 is 0.9037\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "   # Forward propagation\n",
        "   A1, A2, A3 = forward_prop(X_train, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "   #Backward propagation\n",
        "   dW1, db1, dW2, db2, dW3, db3 = backward_prop(X_train, y_train, W1, b1, W2, b2, W3, b3, A1, A2, A3)\n",
        "\n",
        "   #Updating gradients\n",
        "   W1,b1,W2,b2,W3,b3 = update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "   if epoch % 20 == 0:\n",
        "    acc =accuracy_score(A3,y_train)\n",
        "    print(f'The accuracy in {epoch} is {acc}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVjt2XwWdwhE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

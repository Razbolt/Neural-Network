{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM702 - Coursework TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import required libraries for task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "j7joMCD_8U_p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qc1NylZkLXAz",
    "outputId": "a8693de8-48d1-4701-d446-0f3abf0e5555"
   },
   "outputs": [],
   "source": [
    "# Use keras to import MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6uJCuvDsoP2"
   },
   "source": [
    "## 1.2 Data inspection and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4kGn7-Ps25y"
   },
   "source": [
    "Before procesing, we need to check the MNIST Data.\n",
    "First we plot one sample image to see what our input data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "agbg6KpItreT",
    "outputId": "01929e05-179f-45c0-fbcd-70cb54de58a2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number shown on image is: 3\n"
     ]
    }
   ],
   "source": [
    "# Plot a sample image\n",
    "sample = 7\n",
    "image = X_train[sample]\n",
    "# plot the sample\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "print(\"The number shown on image is:\", y_train[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR2Psgt-uJ8d"
   },
   "source": [
    "Then we print the size of the arrays we have created just to make sure they are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZ1--VdWCEzT",
    "outputId": "57913413-7f5e-487a-9ebb-3f1fa2abb060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train (60000, 28, 28)\n",
      "shape of X_test (10000, 28, 28)\n",
      "shape of y_train (60000,)\n",
      "shape of y_test (10000,)\n"
     ]
    }
   ],
   "source": [
    "print (\"shape of X_train {}\".format(X_train.shape))\n",
    "print (\"shape of X_test {}\".format(X_test.shape))\n",
    "print (\"shape of y_train {}\".format(y_train.shape))\n",
    "print (\"shape of y_test {}\".format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6J3kJFKLuWZz"
   },
   "source": [
    "Finally we print one data sample to see what it is like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRu5VJIiuc7j",
    "outputId": "0f485ee9-bbdc-4f7c-c4aa-4a689a74d7b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,  43,\n",
       "        105, 255, 253, 253, 253, 253, 253, 174,   6,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  43, 139, 224, 226,\n",
       "        252, 253, 252, 252, 252, 252, 252, 252, 158,  14,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 178, 252, 252, 252,\n",
       "        252, 253, 252, 252, 252, 252, 252, 252, 252,  59,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 109, 252, 252, 230,\n",
       "        132, 133, 132, 132, 189, 252, 252, 252, 252,  59,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,  29,  29,  24,\n",
       "          0,   0,   0,   0,  14, 226, 252, 252, 172,   7,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,  85, 243, 252, 252, 144,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  88, 189, 252, 252, 252,  14,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  91, 212, 247, 252, 252, 252, 204,   9,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 125, 193, 193,\n",
       "        193, 253, 252, 252, 252, 238, 102,  28,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  45, 222, 252, 252, 252,\n",
       "        252, 253, 252, 252, 252, 177,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  45, 223, 253, 253, 253,\n",
       "        253, 255, 253, 253, 253, 253,  74,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 123,  52,  44,\n",
       "         44,  44,  44, 143, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  15, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  86, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   5,  75,   9,   0,   0,   0,   0,\n",
       "          0,   0,  98, 242, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,  61, 183, 252,  29,   0,   0,   0,   0,\n",
       "         18,  92, 239, 252, 252, 243,  65,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0, 208, 252, 252, 147, 134, 134, 134, 134,\n",
       "        203, 253, 252, 252, 188,  83,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0, 208, 252, 252, 252, 252, 252, 252, 252,\n",
       "        252, 253, 230, 153,   8,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,  49, 157, 252, 252, 252, 252, 252, 217,\n",
       "        207, 146,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   7, 103, 235, 252, 172, 103,  24,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDoduNmWvn2b"
   },
   "source": [
    "We can see that our input is a 28x28 array of integers with values in the range 0 to 255. These correspond to the input image size which is 28x28 pixels and the values represent the colour intensity of each pixel in the image in the greyscale range. 0 is black, 255 is white and the values in between are shades of grey. This can also be confirmed by the image that we have plotted above.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeMY2Oi-y2LS"
   },
   "source": [
    "In order to feed the data into our NN we need to \"flatten\" the 28x28 array into one dimension array containing all 784 elements and preserving the total number of input samples. We print the resulting size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fHFqMAQBRrf",
    "outputId": "ca8af697-9941-4aa7-e283-7ca364b8b6a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new shape of X_train (60000, 784)\n",
      "new shape of X_test (10000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  38,  43, 105, 255, 253,\n",
       "       253, 253, 253, 253, 174,   6,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  43, 139, 224, 226, 252,\n",
       "       253, 252, 252, 252, 252, 252, 252, 158,  14,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 178, 252, 252,\n",
       "       252, 252, 253, 252, 252, 252, 252, 252, 252, 252,  59,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 109,\n",
       "       252, 252, 230, 132, 133, 132, 132, 189, 252, 252, 252, 252,  59,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   4,  29,  29,  24,   0,   0,   0,   0,  14, 226, 252, 252,\n",
       "       172,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  85, 243,\n",
       "       252, 252, 144,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  88,\n",
       "       189, 252, 252, 252,  14,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  91,\n",
       "       212, 247, 252, 252, 252, 204,   9,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 125, 193, 193,\n",
       "       193, 253, 252, 252, 252, 238, 102,  28,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  45, 222, 252,\n",
       "       252, 252, 252, 253, 252, 252, 252, 177,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  45,\n",
       "       223, 253, 253, 253, 253, 255, 253, 253, 253, 253,  74,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  31, 123,  52,  44,  44,  44,  44, 143, 252, 252,  74,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  15, 252,\n",
       "       252,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        86, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   5,  75,   9,   0,   0,   0,   0,   0,\n",
       "         0,  98, 242, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  61, 183, 252,  29,   0,   0,   0,\n",
       "         0,  18,  92, 239, 252, 252, 243,  65,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0, 208, 252, 252, 147, 134,\n",
       "       134, 134, 134, 203, 253, 252, 252, 188,  83,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 208, 252, 252,\n",
       "       252, 252, 252, 252, 252, 252, 253, 230, 153,   8,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  49,\n",
       "       157, 252, 252, 252, 252, 252, 217, 207, 146,  45,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   7, 103, 235, 252, 172, 103,  24,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "print (\"new shape of X_train {}\".format(X_train.shape))\n",
    "print (\"new shape of X_test {}\".format(X_test.shape))\n",
    "X_train[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxCpjwqG1OTe"
   },
   "source": [
    "Since all our features are integer values that range from 0 to 255 it is not absolutely necessary to standardize our input data. However we will perform a simple scaling of the data by dividing all values by the max value 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcReBNIP5Cjs",
    "outputId": "fb7fd9ef-701a-4c7f-92c0-fb76cf424f35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14901961, 0.16862745, 0.41176471, 1.        ,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.68235294, 0.02352941, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.16862745, 0.54509804, 0.87843137,\n",
       "       0.88627451, 0.98823529, 0.99215686, 0.98823529, 0.98823529,\n",
       "       0.98823529, 0.98823529, 0.98823529, 0.98823529, 0.61960784,\n",
       "       0.05490196, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.69803922, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
       "       0.99215686, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
       "       0.98823529, 0.98823529, 0.98823529, 0.23137255, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.42745098, 0.98823529,\n",
       "       0.98823529, 0.90196078, 0.51764706, 0.52156863, 0.51764706,\n",
       "       0.51764706, 0.74117647, 0.98823529, 0.98823529, 0.98823529,\n",
       "       0.98823529, 0.23137255, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.01568627, 0.11372549, 0.11372549, 0.09411765,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.88627451, 0.98823529, 0.98823529, 0.6745098 , 0.02745098,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.33333333, 0.95294118, 0.98823529,\n",
       "       0.98823529, 0.56470588, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.34509804,\n",
       "       0.74117647, 0.98823529, 0.98823529, 0.98823529, 0.05490196,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.35686275, 0.83137255, 0.96862745, 0.98823529, 0.98823529,\n",
       "       0.98823529, 0.8       , 0.03529412, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.1254902 , 0.49019608,\n",
       "       0.75686275, 0.75686275, 0.75686275, 0.99215686, 0.98823529,\n",
       "       0.98823529, 0.98823529, 0.93333333, 0.4       , 0.10980392,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.17647059, 0.87058824, 0.98823529, 0.98823529, 0.98823529,\n",
       "       0.98823529, 0.99215686, 0.98823529, 0.98823529, 0.98823529,\n",
       "       0.69411765, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.17647059, 0.8745098 ,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 1.        ,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.29019608,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.12156863, 0.48235294, 0.20392157,\n",
       "       0.17254902, 0.17254902, 0.17254902, 0.17254902, 0.56078431,\n",
       "       0.98823529, 0.98823529, 0.29019608, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.05882353, 0.98823529, 0.98823529,\n",
       "       0.29019608, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3372549 , 0.98823529, 0.98823529, 0.29019608, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01960784, 0.29411765,\n",
       "       0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.38431373, 0.94901961, 0.98823529,\n",
       "       0.98823529, 0.29019608, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.23921569, 0.71764706, 0.98823529, 0.11372549, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.07058824, 0.36078431,\n",
       "       0.9372549 , 0.98823529, 0.98823529, 0.95294118, 0.25490196,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.81568627, 0.98823529,\n",
       "       0.98823529, 0.57647059, 0.5254902 , 0.5254902 , 0.5254902 ,\n",
       "       0.5254902 , 0.79607843, 0.99215686, 0.98823529, 0.98823529,\n",
       "       0.7372549 , 0.3254902 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.81568627, 0.98823529, 0.98823529, 0.98823529,\n",
       "       0.98823529, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
       "       0.99215686, 0.90196078, 0.6       , 0.03137255, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.19215686,\n",
       "       0.61568627, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
       "       0.98823529, 0.85098039, 0.81176471, 0.57254902, 0.17647059,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.02745098, 0.40392157,\n",
       "       0.92156863, 0.98823529, 0.6745098 , 0.40392157, 0.09411765,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train/255\n",
    "X_train[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFbxcwcENmQk"
   },
   "source": [
    "## 2.1 Build Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6uqi_S-ZPhn9"
   },
   "outputs": [],
   "source": [
    "# We will be building a Neural Network with 2 hidden layers\n",
    "# layer 1 will have 16 nodes, layer 2 will have 16 nodes and the output is 10 nodes\n",
    "\n",
    "def init_params():\n",
    "\n",
    "  #Initialise layer's weights using a uniform distribution in range -0.5 to 0.5\n",
    "  #Initialise bias values to 0\n",
    "  W1 = np.random.uniform(-0.5, 0.5, (16,784))\n",
    "  b1 = np.zeros((16,1))\n",
    "\n",
    "  #Hidden layer's weight\n",
    "  W2 = np.random.uniform(-0.5,0.5, (16,16))\n",
    "  b2 = np.zeros((16,1))\n",
    "\n",
    "  # Second Hidden layer' weight\n",
    "  W3 = np.random.uniform(-0.5,0.5 ,(10,16))  # Weights are still to big\n",
    "  b3 = np.zeros((10,1))\n",
    "\n",
    "  return W1,b1,W2,b2,W3,b3\n",
    "\n",
    "W1,b1,W2,b2,W3,b3 = init_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRw2jsVWH0I4"
   },
   "source": [
    "## 2.2 Build Activation functions and Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBsAF3drU98m",
    "outputId": "338feec9-0fb4-468a-b7bd-2c24981b3517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.66321625e-03 1.61040993e-04 3.59428802e-02 ... 4.45238874e-03\n",
      "  1.31382294e-03 9.30557610e-03]\n",
      " [2.54553916e-03 7.36302258e-04 2.28459513e-01 ... 4.75547115e-03\n",
      "  1.65177875e-03 5.43604363e-03]\n",
      " [3.08501714e-01 2.24470478e-01 9.08149582e-02 ... 3.16821820e-01\n",
      "  2.80183205e-01 9.54753195e-02]\n",
      " ...\n",
      " [5.16808019e-02 4.15957566e-02 1.26416200e-01 ... 3.40730921e-02\n",
      "  1.08086003e-01 3.23472649e-02]\n",
      " [4.33317304e-02 1.05103313e-01 2.42257595e-01 ... 4.65382576e-02\n",
      "  5.40820869e-02 2.78392510e-02]\n",
      " [3.15814524e-01 4.60311157e-01 1.18957151e-01 ... 4.34459989e-01\n",
      "  2.26956461e-01 3.77541796e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Define ReLU function\n",
    "def ReLU(Z):\n",
    "\n",
    "  return np.maximum(0,Z)\n",
    "\n",
    "# Define Softmax\n",
    "def softmax(Z): # CHECK IT !!!\n",
    "\n",
    "  # for one examples is (np.exp(Z) / np.sum( np.exp(Z)))\n",
    "\n",
    "    Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
    "    sum_Z_exp = np.sum(Z_exp, axis=0)\n",
    "    softmax_output = Z_exp / sum_Z_exp\n",
    "\n",
    "    return softmax_output\n",
    "\n",
    "# Define Sigmoid\n",
    "def sigmoid(Z):\n",
    "\n",
    "  return 1 / 1+ np.exp(-Z)\n",
    "\n",
    "def forward_prop(X,W1,b1,W2,b2,W3,b3,dropout = 0.0):\n",
    "  #We are assuming to have 3 layered neural network \n",
    "  #D1 and D2 are mask layers for dropout \n",
    "\n",
    "  D1= None\n",
    "  D2= None\n",
    "\n",
    "  # if there is no dropout layer\n",
    "  if (dropout) == 0.0:\n",
    "    #Forward propagation with 3 layered neural network\n",
    "\n",
    "    #Initialize  Z\n",
    "    #Since X has shape (6000,784) and our W1 has shape (16,784) for the dot product we need to use Transpose of X\n",
    "\n",
    "    Z1 = W1.dot(X.T)+ b1\n",
    "    #Activation with ReLU to given linear regression to feed neural network\n",
    "    A1 = ReLU(Z1)\n",
    "\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "\n",
    "    A2 = ReLU(Z2)\n",
    "\n",
    "    Z3 = W3.dot(A2) + b3\n",
    "    #We use softmax at the output layer to have probabilistic set of values\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    " \n",
    "\n",
    "  else: # If there is a dropout layer, we are going to build mask for each layer\n",
    "    #Forward propagation with 3 layered neural network \n",
    "\n",
    "    Z1 = W1.dot(X.T)+ b1\n",
    "    #Activation with ReLU to given linear regression to feed neural network\n",
    "    A1 = ReLU(Z1)\n",
    "    '''\n",
    "    This line of code for dropout inspired from Andrew Ng's Deep Learning Specialization Course\n",
    "    https://www.youtube.com/watch?v=D8PJAL-MZv8\n",
    "    '''\n",
    "    #Create a mask for A1 to use probability of dropout from user\n",
    "    D1 = np.random.rand(*A1.shape) > dropout \n",
    "    #D1 is mask matrix that checks the probability of dropout.\n",
    "    #If the probability is bigger than dropout, it will be 1 otherwise 0\n",
    "\n",
    "    #Apply mask to A1\n",
    "    A1 = A1 * D1\n",
    "    #Normalize A1 to not change expected value of A1 \n",
    "    A1 = A1 / (1-dropout) \n",
    "    # It scales A to not to change expected value of A1 as keeping  probability \n",
    "    #keeeping probability means 1 - dropout probability\n",
    "\n",
    "   \n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = ReLU(Z2)\n",
    "    \n",
    "    #Create a mask for second hidden layer and use probability of dropout from user\n",
    "    D2 = np.random.rand( *A2.shape) > dropout\n",
    "    A2 = A2 * D2\n",
    "    A2 = A2 / (1-dropout)\n",
    "\n",
    "    Z3 = W3.dot(A2) + b3\n",
    "    #At the end use softmax in the output layer to obtain probabilistic values\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "  return A1,A2,A3,D1,D2\n",
    "\n",
    "A1,A2,A3,D1,D2 = forward_prop(X_train,W1,b1,W2,b2,W3,b3)\n",
    "\n",
    "print(A3)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(A1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFn_mUjHPKab"
   },
   "source": [
    "## 2.3 Back-Propagation with One-hot and derivative of activaton functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5dhSX3T_HAx9"
   },
   "outputs": [],
   "source": [
    "def one_hot(Y): # CHECK IT !!\n",
    "  num_classes = np.max(Y) + 1\n",
    "\n",
    "  one_hot = np.zeros((Y.size,num_classes))\n",
    "  one_hot[np.arange(Y.size), Y] = 1\n",
    "\n",
    "  return one_hot.T\n",
    "\n",
    "def derivative_ReLU(Z):\n",
    "  #Do we need to check between 0-1 ?\n",
    "  return Z > 0\n",
    "\n",
    "#Focus on backpropagation !!! Check how its works ! \n",
    "def backward_prop(X, Y, W1, b1, W2, b2, W3, b3, A1, A2, A3,D1, D2, dropout = 0.0): # CHECK IT !!!!\n",
    "\n",
    "  m = Y.size\n",
    "  Y = one_hot(Y)\n",
    "  #if dropout is not used\n",
    "\n",
    "  if (dropout) == 0.0:\n",
    "    \n",
    "    #One hot encoded to see each labels in matrix as 1\n",
    "    \n",
    "\n",
    "    #Derivate of cost function with respect to z3\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
    "    dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
    "\n",
    "    #Derivate for second hidden layer\n",
    "    dA2 = np.dot(W3.T,dZ3)\n",
    "    dZ2 =  dA2  * derivative_ReLU(A2)\n",
    "    dW2 = (1/ m) * dZ2.dot(A1.T)\n",
    "    dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
    "\n",
    "    #Derivate for first hidden layer\n",
    "\n",
    "    dA1 = np.dot(W2.T,dZ2)\n",
    "    dZ1 = dA1 * derivative_ReLU(A1)\n",
    "    dW1 = (1/m) * dZ1.dot(X)\n",
    "    dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
    "\n",
    "  else: #If dropout is used\n",
    "\n",
    "    #Derivate of cost function with respect to z3\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
    "    dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
    "\n",
    "    #Derivate for second hidden layer\n",
    "    dA2 = np.dot(W3.T,dZ3)\n",
    "    #Dropout mask for second hidden layer\n",
    "    dA2 = dA2 * (D2)\n",
    "    #dA2 = D2 / dropout\n",
    "\n",
    "    dZ2 =  dA2  * derivative_ReLU(A2)\n",
    "    dW2 = (1/ m) * dZ2.dot(A1.T)\n",
    "    dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
    "\n",
    "    #Derivate for first hidden layer\n",
    "\n",
    "    dA1 = np.dot(W2.T,dZ2)\n",
    "    #Dropout mask for first hidden layer\n",
    "    dA1 = dA1 * (D1)\n",
    "    #dA1 = D1 / dropout\n",
    "\n",
    "    dZ1 = dA1 * derivative_ReLU(A1)\n",
    "    dW1 = (1/m) * dZ1.dot(X)\n",
    "    dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
    "    \n",
    "\n",
    "  return dW1,dB1,dW2,dB2,dW3,dB3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tYISPvOQRQn"
   },
   "source": [
    "## 2.4 Update the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MFbTDfoWQYq-"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs =  1500\n",
    "\n",
    "def update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate):\n",
    "\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "\n",
    "    return W1,b1,W2,b2,W3,b3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mhb1TpDrQtKt"
   },
   "source": [
    "## 2.5 Try in given epochs time to see the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "i-QTIprNdkBP"
   },
   "outputs": [],
   "source": [
    "def accuracy_score(A,Y):\n",
    "    size = Y.size\n",
    "    predict = np.argmax(A,0)\n",
    "\n",
    "    correct = np.sum(predict == Y)\n",
    "\n",
    "    accuracy = correct / size\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "#print(predictions.max)\n",
    "#print(y_train)\n",
    "#print(np.sum(y_train == predictions))\n",
    "#print(y_train.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLlbWaJGN7Pw",
    "outputId": "eecb592c-7f38-4726-8423-b3297eaa559c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy in epoch 0 is 0.8106166666666667\n",
      "The accuracy in epoch 100 is 0.8143666666666667\n",
      "The accuracy in epoch 200 is 0.81875\n",
      "The accuracy in epoch 300 is 0.8215166666666667\n",
      "The accuracy in epoch 400 is 0.8282666666666667\n",
      "The accuracy in epoch 500 is 0.828\n",
      "The accuracy in epoch 600 is 0.8324166666666667\n",
      "The accuracy in epoch 700 is 0.8333666666666667\n",
      "The accuracy in epoch 800 is 0.8375833333333333\n",
      "The accuracy in epoch 900 is 0.83965\n",
      "The accuracy in epoch 1000 is 0.8401833333333333\n",
      "The accuracy in epoch 1100 is 0.8427166666666667\n",
      "The accuracy in epoch 1200 is 0.8470333333333333\n",
      "The accuracy in epoch 1300 is 0.8493166666666667\n",
      "The accuracy in epoch 1400 is 0.84825\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.1\n",
    "for epoch in range(epochs):\n",
    "   # Forward propagation\n",
    "   A1, A2, A3,D1,D2 = forward_prop(X_train, W1, b1, W2, b2, W3, b3,dropout=dropout)\n",
    "\n",
    "   #Backward propagation\n",
    "   dW1, db1, dW2, db2, dW3, db3 = backward_prop(X_train, y_train, W1, b1, W2, b2, W3, b3, A1, A2, A3,D1,D2,dropout=dropout)\n",
    "\n",
    "   #Updating gradients\n",
    "   W1,b1,W2,b2,W3,b3 = update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "   if epoch % 100 == 0:\n",
    "    acc =accuracy_score(A3,y_train)\n",
    "    print(f'The accuracy in epoch {epoch} is {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Connected Class Version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey Greg Please check this part! \n",
    "\n",
    "I havent updated with comments because most of them written in above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Missing parts\n",
    "\n",
    "\n",
    "\n",
    "- part d)\n",
    "\n",
    "    1. update rule\n",
    "    2. decay\n",
    "    3. L1 OR L2 regularizator\n",
    "\n",
    "\n",
    "- part e) Optimizer\n",
    "\n",
    "    3. mini-batch gradient\n",
    "    4. stoacstic- graident descent \n",
    "\n",
    "May need to check how these gonna change the functions: train, forward and backward prop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self,input_size,hidden_size,output_size,activation_function,dropout):\n",
    "        self.activation_function = activation_function\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.W1 = np.random.uniform(-0.5,0.5,(hidden_size,input_size))\n",
    "        self.b1 = np.zeros((hidden_size,1))\n",
    "\n",
    "        self.W2 = np.random.uniform(-0.5,0.5,(hidden_size,hidden_size))\n",
    "        self.b2 = np.zeros((hidden_size,1))\n",
    "\n",
    "        self.W3 = np.random.uniform(-0.5,0.5,(output_size,hidden_size))\n",
    "        self.b3 = np.zeros((output_size,1))\n",
    "    \n",
    "    #Staticmethod is used to call the function without creating an object\n",
    "    #In this way we can call them in the activation function and deactivation function\n",
    "    @staticmethod  \n",
    "    def ReLU(Z):\n",
    "        return np.maximum(0,Z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_ReLU(Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(Z):\n",
    "        return MyNeuralNetwork.sigmoid(Z) * (1 - MyNeuralNetwork.sigmoid(Z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(Z):\n",
    "        Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
    "        sum_Z_exp = np.sum(Z_exp, axis=0)\n",
    "        softmax_output = Z_exp / sum_Z_exp\n",
    "        return softmax_output\n",
    "    \n",
    "\n",
    "    \n",
    "    def one_hot(self,Y):\n",
    "        num_classes = np.max(Y) + 1\n",
    "        one_hot = np.zeros((Y.size,num_classes))\n",
    "        one_hot[np.arange(Y.size), Y] = 1\n",
    "        return one_hot.T\n",
    "    \n",
    "    \n",
    "    def activation(self,Z):\n",
    "        if self.activation_function == 'relu':\n",
    "            return self.ReLU(Z)\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            return self.sigmoid(Z)\n",
    "        elif self.activation_function == 'softmax':\n",
    "            return self.softmax(Z)\n",
    "        else:\n",
    "            raise Exception('Activation function not supported')\n",
    "        \n",
    "    def derivative_activation(self,A):\n",
    "        if self.activation_function == 'relu':\n",
    "            return self.derivative_ReLU(A)\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            return self.derivative_sigmoid(A)\n",
    "        else:\n",
    "            raise Exception('Activation function not supported')\n",
    "    \n",
    "    def forward_prop(self,X,dropout):\n",
    "        D1 = None\n",
    "        D2 = None\n",
    "        #Forward propagation with 3 layered neural network\n",
    "        \n",
    "        if dropout == 0.0: # If there is no dropout layer\n",
    "            Z1 = self.W1.dot(X.T) + self.b1\n",
    "            A1 = self.activation(Z1)\n",
    "\n",
    "            Z2 = self.W2.dot(A1) + self.b2\n",
    "            A2 = self.activation(Z2)\n",
    "\n",
    "            Z3 = self.W3.dot(A2) + self.b3\n",
    "            A3 = self.softmax(Z3)\n",
    "        else:              # If there is a dropout layer \n",
    "            \"\"\" This line of code for dropout inspired from Andrew Ng's Deep Learning Specialization Course\"\"\"\n",
    "            \"\"\" https://www.youtube.com/watch?v=D8PJAL-MZv8 \"\"\"\n",
    "\n",
    "            # Create a mask for A1 as use probability of dropout from user\n",
    "            Z1 = self.W1.dot(X.T) + self.b1 \n",
    "            A1 = self.activation(Z1)\n",
    "    \n",
    "            D1 = np.random.rand(*A1.shape) > dropout # D1 is mask matrix that check the proability of dropout.\n",
    "            A1 = A1 * D1                           # If the probability is bigger than dropout, it will be 1 otherwise 0\n",
    "            A1 = A1 / (1 - dropout)                # It scales A to not to change expected value of A1 as keeping  probability\n",
    "\n",
    "            Z2 = self.W2.dot(A1) + self.b2\n",
    "            A2 = self.activation(Z2)\n",
    "\n",
    "            D2 = np.random.rand(*A2.shape) > dropout\n",
    "            A2 = A2 * D2\n",
    "            A2 = A2 / (1 - dropout)\n",
    "\n",
    "            Z3 = self.W3.dot(A2) + self.b3 \n",
    "            A3 = self.softmax(Z3) # At the end softmax  the output layer to have a probabilistic values\n",
    "        \n",
    "        return A1,A2,A3,D1,D2\n",
    "    \n",
    "    def backward_prop(self,X,Y,A1,A2,A3,D1,D2,dropout):\n",
    "        m = Y.size\n",
    "        Y = self.one_hot(Y)\n",
    "        \n",
    "        # Derivate of cost function with respect to z3\n",
    "        dZ3 = A3 - Y\n",
    "        dW3 = (1/m) * dZ3.dot(A2.T)\n",
    "        db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "        # Derivate for second hidden layer\n",
    "        dA2 = np.dot(self.W3.T, dZ3)\n",
    "        if dropout > 0:\n",
    "            dA2 = dA2 * D2  # Apply dropout\n",
    "        dZ2 = dA2 * self.derivative_activation(A2)\n",
    "        dW2 = (1/m) * dZ2.dot(A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        # Derivate for first hidden layer\n",
    "        dA1 = np.dot(self.W2.T, dZ2)\n",
    "        if dropout > 0:\n",
    "            dA1 = dA1 * D1  # Apply dropout\n",
    "        dZ1 = dA1 * self.derivative_activation(A1)\n",
    "        dW1 = (1/m) * dZ1.dot(X)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        # Store the gradients\n",
    "        self.dW1 = dW1\n",
    "        self.db1 = db1\n",
    "        self.dW2 = dW2\n",
    "        self.db2 = db2\n",
    "        self.dW3 = dW3\n",
    "        self.db3 = db3\n",
    "\n",
    "    def update_gradient(self,learning_rate):\n",
    "        self.W1 -= learning_rate * self.dW1\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "        self.W2 -= learning_rate * self.dW2\n",
    "        self.b2 -= learning_rate * self.db2\n",
    "        self.W3 -= learning_rate * self.dW3\n",
    "        self.b3 -= learning_rate * self.db3\n",
    "\n",
    "\n",
    "    def train(self,X,Y,learning_rate,epochs): # Need some updates based on batch size, mini-batch  etc.\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            A1, A2, A3, D1, D2 = self.forward_prop(X, self.dropout)\n",
    "\n",
    "            # Backward propagation\n",
    "            self.backward_prop(X, Y, A1, A2, A3, D1, D2, self.dropout)\n",
    "\n",
    "            # Updating gradients\n",
    "            self.update_gradient(learning_rate)\n",
    "\n",
    "\n",
    "    def predict(self,X):\n",
    "        A1, A2, A3, D1, D2 = self.forward_prop(X, dropout=0.0)\n",
    "        return np.argmax(A3, axis=0)\n",
    "\n",
    "    def accuracy_score(self,X,Y):\n",
    "        size = Y.size\n",
    "        predict = self.predict(X)\n",
    "        correct = np.sum(predict == Y)\n",
    "        accuracy = correct / size\n",
    "        return accuracy\n",
    "\n",
    "    def test(self,X,Y):\n",
    "        accuracy = self.accuracy_score(X,Y)\n",
    "        print(f'The accuracy is {accuracy*100}')\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MyNeuralNetwork(input_size=784, hidden_size=30, output_size=10, activation_function='relu',dropout=0.5)\n",
    "\n",
    "nn.train(X_train, y_train, learning_rate=0.2, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 69.37\n"
     ]
    }
   ],
   "source": [
    "nn.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Razbolt/Neural-Network/blob/main/Neural_Network_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGR6imnPESL"
      },
      "source": [
        "#1.1 Install the MNIST and requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j7joMCD_8U_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import load_digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9rb-9S-KJrD",
        "outputId": "67a98d93-63de-44df-afcf-2b20f8dfb497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-mnist\n",
            "  Using cached python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement quiet (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for quiet\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install python-mnist -- quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qc1NylZkLXAz"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "# Using keras libraries to take MNIST dataset\n",
        "(X_train, y_train,), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1 Data preparation"
      ],
      "metadata": {
        "id": "C6uJCuvDsoP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before procesing, we need to check the MNIST Data.\n",
        "\n",
        "As we can see the X_train data is numbered from 0 to 255. In order to use them we need to normalize them as dividing to 255 each of the value."
      ],
      "metadata": {
        "id": "R4kGn7-Ps25y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ1--VdWCEzT",
        "outputId": "69f3a8c0-361f-42e4-ea3b-07d7a0d456a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to check the shapes before feeding them into model."
      ],
      "metadata": {
        "id": "2UIxpHoosaud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJs76_reOzPj",
        "outputId": "58d2aaa7-8d23-4874-ebe1-d9db9bec039c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zdUnLliLXu2",
        "outputId": "a875fade-abc2-4672-e997-f31a22e726a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9u7khFALZFa",
        "outputId": "c548320b-5844-426d-acd8-87af98bae5e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "X_train[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fHFqMAQBRrf",
        "outputId": "e16a193b-4b11-4f45-b77b-404151b3117e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784) (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Since we see the shapes are (6000,28,28) we need to reshape them to feed our model\n",
        "\n",
        "X_train = X_train.reshape(60000, 784) / 255.0\n",
        "\n",
        "X_test = X_test.reshape(10000, 784) /255.0\n",
        "\n",
        "print(X_train.shape , X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2e-p43TCRRW",
        "outputId": "00e7aa2a-5fd1-446f-ab04-35a963306807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
            " 0.96862745 0.49803922 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
            " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.19215686\n",
            " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
            " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
            " 0.96862745 0.94509804 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
            " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.04313725\n",
            " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.1372549  0.94509804\n",
            " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
            " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
            " 0.58823529 0.10588235 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
            " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15294118 0.58039216\n",
            " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.67058824\n",
            " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
            " 0.31372549 0.03529412 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.53333333 0.99215686\n",
            " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFbxcwcENmQk"
      },
      "source": [
        "#2.1 Build Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "6uqi_S-ZPhn9"
      },
      "outputs": [],
      "source": [
        "def init_params():\n",
        "\n",
        "  #input layer's weigths\n",
        "  W1 = np.random.uniform(-0.5, 0.5, (16,784))\n",
        "  b1 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Hidden layer's weight\n",
        "  W2 = np.random.uniform(-0.5,0.5, (16,16))\n",
        "  b2 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "  # Second Hidden layer' weight\n",
        "  W3 = np.random.uniform(-0.5,0.5 ,(10,16))  # Weights are still to big\n",
        "  b3 = np.zeros((10,1))\n",
        "\n",
        "\n",
        "  return W1,b1,W2,b2,W3,b3\n",
        "\n",
        "\n",
        "\n",
        "W1,b1,W2,b2,W3,b3 = init_params()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRw2jsVWH0I4"
      },
      "source": [
        "#1.2 Build Activation functions and Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBsAF3drU98m",
        "outputId": "338feec9-0fb4-468a-b7bd-2c24981b3517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00679114 0.00527258 0.0658074  ... 0.00207419 0.00600942 0.03091759]\n",
            " [0.01790551 0.28869534 0.05381954 ... 0.18655483 0.14435255 0.18302996]\n",
            " [0.78716232 0.30405957 0.36980117 ... 0.50645333 0.02556571 0.22413396]\n",
            " ...\n",
            " [0.0109231  0.03390429 0.05191376 ... 0.04565613 0.09377314 0.06274774]\n",
            " [0.00242549 0.00778729 0.02485256 ... 0.00833378 0.00270395 0.09929324]\n",
            " [0.02089471 0.00800702 0.0968669  ... 0.00419734 0.00102753 0.06981688]]\n"
          ]
        }
      ],
      "source": [
        "def ReLU(Z):\n",
        "\n",
        "  return np.maximum(0,Z)\n",
        "\n",
        "\n",
        "def softmax(Z): # CHECK IT !!!\n",
        "\n",
        "  # for one examples is (np.exp(Z) / np.sum( np.exp(Z)))\n",
        "\n",
        "    Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
        "    sum_Z_exp = np.sum(Z_exp, axis=0)\n",
        "    softmax_output = Z_exp / sum_Z_exp\n",
        "\n",
        "    return softmax_output\n",
        "\n",
        "def sigmoid(Z):\n",
        "\n",
        "  return 1 / 1+ np.exp(-Z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def forward_prop(X,W1,b1,W2,b2,W3,b3):\n",
        "  #Forward propagation with 3 layered neural network\n",
        "\n",
        "  #Initialize  Z\n",
        "  #Since X is shape as (6000,784) and our W1 is shape as (16,784) in order to dot product we need to take Transpose of X\n",
        "\n",
        "  Z1 = W1.dot(X.T)+ b1\n",
        "  #Activation with ReLU to given linear regression to feed neural network\n",
        "  A1 = ReLU(Z1)\n",
        "\n",
        "  Z2 = W2.dot(A1) + b2\n",
        "\n",
        "\n",
        "  A2 = ReLU(Z2)\n",
        "\n",
        "  Z3 = W3.dot(A2) + b3\n",
        "  #At the end softmax  the output layer to have a probabilistic values\n",
        "  A3 = softmax(Z3)\n",
        "\n",
        "  return A1,A2,A3\n",
        "\n",
        "A1,A2,A3 = forward_prop(X_train,W1,b1,W2,b2,W3,b3)\n",
        "\n",
        "print(A3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFn_mUjHPKab"
      },
      "source": [
        "#1.3 Back-Propagation with One-hot and derivative of activaton functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5dhSX3T_HAx9"
      },
      "outputs": [],
      "source": [
        "def one_hot(Y): # CHECK IT !!\n",
        "  num_classes = np.max(Y) + 1\n",
        "\n",
        "  one_hot = np.zeros((Y.size,num_classes))\n",
        "  one_hot[np.arange(Y.size), Y] = 1\n",
        "\n",
        "  return one_hot.T\n",
        "\n",
        "def derivative_ReLU(Z):\n",
        "  #Do we need to check between 0-1 ?\n",
        "  return Z > 0\n",
        "\n",
        "\n",
        "def backward_prop(X, Y, W1, b1, W2, b2, W3, b3, A1, A2, A3): # CHECK IT !!!!\n",
        "\n",
        "  m = Y.size\n",
        "  #One hot encoded to see each labels in matrix as 1\n",
        "  Y = one_hot(Y)\n",
        "\n",
        "  #Derivate of cost function with respect to z3\n",
        "  dZ3 = A3 - Y\n",
        "  dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "  dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
        "\n",
        "  #Derivate for second hidden layer\n",
        "  dA2 = np.dot(W3.T,dZ3)\n",
        "  dZ2 =  dA2  * derivative_ReLU(A2)\n",
        "  dW2 = (1/ m) * dZ2.dot(A1.T)\n",
        "  dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
        "\n",
        "  #Derivate for first hidden layer\n",
        "\n",
        "  dA1 = np.dot(W2.T,dZ2)\n",
        "  dZ1 = dA1 * derivative_ReLU(A1)\n",
        "  dW1 = (1/m) * dZ1.dot(X)\n",
        "  dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
        "\n",
        "  return dW1,dB1,dW2,dB2,dW3,dB3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tYISPvOQRQn"
      },
      "source": [
        "#1.3 Update the Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "MFbTDfoWQYq-"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "epochs =  1000\n",
        "\n",
        "def update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate):\n",
        "\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "\n",
        "    return W1,b1,W2,b2,W3,b3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhb1TpDrQtKt"
      },
      "source": [
        "#1.4 Try in given epochs time to see the changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "i-QTIprNdkBP"
      },
      "outputs": [],
      "source": [
        "def accuracy_score(A,Y):\n",
        "    size = Y.size\n",
        "    predict = np.argmax(A,0)\n",
        "\n",
        "    correct = np.sum(predict == Y)\n",
        "\n",
        "    accuracy = correct / size\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "#print(predictions.max)\n",
        "#print(y_train)\n",
        "#print(np.sum(y_train == predictions))\n",
        "#print(y_train.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLlbWaJGN7Pw",
        "outputId": "eecb592c-7f38-4726-8423-b3297eaa559c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy in 0 is 0.6655\n",
            "The accuracy in 20 is 0.7005333333333333\n",
            "The accuracy in 40 is 0.72825\n",
            "The accuracy in 60 is 0.7502833333333333\n",
            "The accuracy in 80 is 0.7692\n",
            "The accuracy in 100 is 0.7851666666666667\n",
            "The accuracy in 120 is 0.7992333333333334\n",
            "The accuracy in 140 is 0.81045\n",
            "The accuracy in 160 is 0.8194833333333333\n",
            "The accuracy in 180 is 0.8268333333333333\n",
            "The accuracy in 200 is 0.8329166666666666\n",
            "The accuracy in 220 is 0.8385333333333334\n",
            "The accuracy in 240 is 0.843\n",
            "The accuracy in 260 is 0.8265666666666667\n",
            "The accuracy in 280 is 0.8520166666666666\n",
            "The accuracy in 300 is 0.8557333333333333\n",
            "The accuracy in 320 is 0.8584833333333334\n",
            "The accuracy in 340 is 0.86105\n",
            "The accuracy in 360 is 0.86335\n",
            "The accuracy in 380 is 0.8658833333333333\n",
            "The accuracy in 400 is 0.86815\n",
            "The accuracy in 420 is 0.8708166666666667\n",
            "The accuracy in 440 is 0.8726833333333334\n",
            "The accuracy in 460 is 0.87495\n",
            "The accuracy in 480 is 0.87665\n",
            "The accuracy in 500 is 0.8784666666666666\n",
            "The accuracy in 520 is 0.8802833333333333\n",
            "The accuracy in 540 is 0.8818\n",
            "The accuracy in 560 is 0.8833666666666666\n",
            "The accuracy in 580 is 0.8850166666666667\n",
            "The accuracy in 600 is 0.8861333333333333\n",
            "The accuracy in 620 is 0.8873\n",
            "The accuracy in 640 is 0.8884166666666666\n",
            "The accuracy in 660 is 0.8898166666666667\n",
            "The accuracy in 680 is 0.8907666666666667\n",
            "The accuracy in 700 is 0.89175\n",
            "The accuracy in 720 is 0.8929666666666667\n",
            "The accuracy in 740 is 0.89385\n",
            "The accuracy in 760 is 0.8947166666666667\n",
            "The accuracy in 780 is 0.8958833333333334\n",
            "The accuracy in 800 is 0.8969666666666667\n",
            "The accuracy in 820 is 0.8978\n",
            "The accuracy in 840 is 0.8985333333333333\n",
            "The accuracy in 860 is 0.8991833333333333\n",
            "The accuracy in 880 is 0.9001333333333333\n",
            "The accuracy in 900 is 0.9009833333333334\n",
            "The accuracy in 920 is 0.90185\n",
            "The accuracy in 940 is 0.9025333333333333\n",
            "The accuracy in 960 is 0.9030666666666667\n",
            "The accuracy in 980 is 0.9037\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "   # Forward propagation\n",
        "   A1, A2, A3 = forward_prop(X_train, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "   #Backward propagation\n",
        "   dW1, db1, dW2, db2, dW3, db3 = backward_prop(X_train, y_train, W1, b1, W2, b2, W3, b3, A1, A2, A3)\n",
        "\n",
        "   #Updating gradients\n",
        "   W1,b1,W2,b2,W3,b3 = update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "   if epoch % 20 == 0:\n",
        "    acc =accuracy_score(A3,y_train)\n",
        "    print(f'The accuracy in {epoch} is {acc}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "gVjt2XwWdwhE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA4B2Ij65Zu0Sldqt5uFEq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
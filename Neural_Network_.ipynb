{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Razbolt/Neural-Network/blob/greg/Neural_Network_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGR6imnPESL"
      },
      "source": [
        "#1.1 Import the libraries we need and load the dataset using Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "j7joMCD_8U_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qc1NylZkLXAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8693de8-48d1-4701-d446-0f3abf0e5555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Use keras to import MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Data inspection and preparation"
      ],
      "metadata": {
        "id": "C6uJCuvDsoP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before procesing, we need to check the MNIST Data.\n",
        "First we plot one sample image to see what our input data look like"
      ],
      "metadata": {
        "id": "R4kGn7-Ps25y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a sample image\n",
        "sample = 7\n",
        "image = X_train[sample]\n",
        "# plot the sample\n",
        "fig = plt.figure\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show()\n",
        "print(y_train[sample])"
      ],
      "metadata": {
        "id": "agbg6KpItreT",
        "outputId": "01929e05-179f-45c0-fbcd-70cb54de58a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we print the size of the arrays we have created just to make sure they are as expected"
      ],
      "metadata": {
        "id": "wR2Psgt-uJ8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ1--VdWCEzT",
        "outputId": "57913413-7f5e-487a-9ebb-3f1fa2abb060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of X_train (60000, 28, 28)\n",
            "shape of X_test (10000, 28, 28)\n",
            "shape of y_train (60000,)\n",
            "shape of y_test (10000,)\n"
          ]
        }
      ],
      "source": [
        "print (\"shape of X_train {}\".format(X_train.shape))\n",
        "print (\"shape of X_test {}\".format(X_test.shape))\n",
        "print (\"shape of y_train {}\".format(y_train.shape))\n",
        "print (\"shape of y_test {}\".format(y_test.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we print one data sample to see what it is like"
      ],
      "metadata": {
        "id": "6J3kJFKLuWZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[7]"
      ],
      "metadata": {
        "id": "eRu5VJIiuc7j",
        "outputId": "0f485ee9-bbdc-4f7c-c4aa-4a689a74d7b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,  43,\n",
              "        105, 255, 253, 253, 253, 253, 253, 174,   6,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  43, 139, 224, 226,\n",
              "        252, 253, 252, 252, 252, 252, 252, 252, 158,  14,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 178, 252, 252, 252,\n",
              "        252, 253, 252, 252, 252, 252, 252, 252, 252,  59,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 109, 252, 252, 230,\n",
              "        132, 133, 132, 132, 189, 252, 252, 252, 252,  59,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,  29,  29,  24,\n",
              "          0,   0,   0,   0,  14, 226, 252, 252, 172,   7,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,  85, 243, 252, 252, 144,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  88, 189, 252, 252, 252,  14,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  91, 212, 247, 252, 252, 252, 204,   9,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 125, 193, 193,\n",
              "        193, 253, 252, 252, 252, 238, 102,  28,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  45, 222, 252, 252, 252,\n",
              "        252, 253, 252, 252, 252, 177,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  45, 223, 253, 253, 253,\n",
              "        253, 255, 253, 253, 253, 253,  74,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 123,  52,  44,\n",
              "         44,  44,  44, 143, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  15, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,  86, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   5,  75,   9,   0,   0,   0,   0,\n",
              "          0,   0,  98, 242, 252, 252,  74,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,  61, 183, 252,  29,   0,   0,   0,   0,\n",
              "         18,  92, 239, 252, 252, 243,  65,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0, 208, 252, 252, 147, 134, 134, 134, 134,\n",
              "        203, 253, 252, 252, 188,  83,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0, 208, 252, 252, 252, 252, 252, 252, 252,\n",
              "        252, 253, 230, 153,   8,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,  49, 157, 252, 252, 252, 252, 252, 217,\n",
              "        207, 146,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   7, 103, 235, 252, 172, 103,  24,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our input is a 28x28 array of integers with values in the range 0 to 255. These correspond to the input image size which is 28x28 pixels and the values represent the colour intensity of each pixel in the image in the greyscale range. 0 is black, 255 is white and the values in between are shades of grey. This can also be confirmed by the image that we have plotted above.   "
      ],
      "metadata": {
        "id": "ZDoduNmWvn2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to feed the data into our NN we need to \"flatten\" the 28x28 array into one dimension array containing all 784 elements and preserving the total number of input samples. We print the resulting size."
      ],
      "metadata": {
        "id": "VeMY2Oi-y2LS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fHFqMAQBRrf",
        "outputId": "ca8af697-9941-4aa7-e283-7ca364b8b6a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of X_train_flat (60000, 784)\n",
            "shape of X_test_flat (10000, 784)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  38,  43, 105, 255, 253,\n",
              "       253, 253, 253, 253, 174,   6,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  43, 139, 224, 226, 252,\n",
              "       253, 252, 252, 252, 252, 252, 252, 158,  14,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 178, 252, 252,\n",
              "       252, 252, 253, 252, 252, 252, 252, 252, 252, 252,  59,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 109,\n",
              "       252, 252, 230, 132, 133, 132, 132, 189, 252, 252, 252, 252,  59,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   4,  29,  29,  24,   0,   0,   0,   0,  14, 226, 252, 252,\n",
              "       172,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  85, 243,\n",
              "       252, 252, 144,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  88,\n",
              "       189, 252, 252, 252,  14,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  91,\n",
              "       212, 247, 252, 252, 252, 204,   9,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,  32, 125, 193, 193,\n",
              "       193, 253, 252, 252, 252, 238, 102,  28,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  45, 222, 252,\n",
              "       252, 252, 252, 253, 252, 252, 252, 177,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  45,\n",
              "       223, 253, 253, 253, 253, 255, 253, 253, 253, 253,  74,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,  31, 123,  52,  44,  44,  44,  44, 143, 252, 252,  74,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  15, 252,\n",
              "       252,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        86, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   5,  75,   9,   0,   0,   0,   0,   0,\n",
              "         0,  98, 242, 252, 252,  74,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,  61, 183, 252,  29,   0,   0,   0,\n",
              "         0,  18,  92, 239, 252, 252, 243,  65,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0, 208, 252, 252, 147, 134,\n",
              "       134, 134, 134, 203, 253, 252, 252, 188,  83,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 208, 252, 252,\n",
              "       252, 252, 252, 252, 252, 252, 253, 230, 153,   8,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  49,\n",
              "       157, 252, 252, 252, 252, 252, 217, 207, 146,  45,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   7, 103, 235, 252, 172, 103,  24,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "X_test_flat = X_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "print (\"shape of X_train_flat {}\".format(X_train_flat.shape))\n",
        "print (\"shape of X_test_flat {}\".format(X_test_flat.shape))\n",
        "X_train_flat[7]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since all our features are integer values that range from 0 to 255 it is not absolutely necessary to standardize our input data. However we will perform a simple scaling of the data by dividing all values by the max value 255"
      ],
      "metadata": {
        "id": "UxCpjwqG1OTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = X_train_flat/255\n",
        "X_train_scaled[7]"
      ],
      "metadata": {
        "id": "pcReBNIP5Cjs",
        "outputId": "fb7fd9ef-701a-4c7f-92c0-fb76cf424f35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.14901961, 0.16862745, 0.41176471, 1.        ,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.68235294, 0.02352941, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.16862745, 0.54509804, 0.87843137,\n",
              "       0.88627451, 0.98823529, 0.99215686, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.98823529, 0.98823529, 0.98823529, 0.61960784,\n",
              "       0.05490196, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.69803922, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.99215686, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.98823529, 0.98823529, 0.23137255, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.42745098, 0.98823529,\n",
              "       0.98823529, 0.90196078, 0.51764706, 0.52156863, 0.51764706,\n",
              "       0.51764706, 0.74117647, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.23137255, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.01568627, 0.11372549, 0.11372549, 0.09411765,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
              "       0.88627451, 0.98823529, 0.98823529, 0.6745098 , 0.02745098,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.33333333, 0.95294118, 0.98823529,\n",
              "       0.98823529, 0.56470588, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.34509804,\n",
              "       0.74117647, 0.98823529, 0.98823529, 0.98823529, 0.05490196,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.35686275, 0.83137255, 0.96862745, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.8       , 0.03529412, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.1254902 , 0.49019608,\n",
              "       0.75686275, 0.75686275, 0.75686275, 0.99215686, 0.98823529,\n",
              "       0.98823529, 0.98823529, 0.93333333, 0.4       , 0.10980392,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.17647059, 0.87058824, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.99215686, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.69411765, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.17647059, 0.8745098 ,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 1.        ,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.29019608,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.12156863, 0.48235294, 0.20392157,\n",
              "       0.17254902, 0.17254902, 0.17254902, 0.17254902, 0.56078431,\n",
              "       0.98823529, 0.98823529, 0.29019608, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.05882353, 0.98823529, 0.98823529,\n",
              "       0.29019608, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.3372549 , 0.98823529, 0.98823529, 0.29019608, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.01960784, 0.29411765,\n",
              "       0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.38431373, 0.94901961, 0.98823529,\n",
              "       0.98823529, 0.29019608, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.23921569, 0.71764706, 0.98823529, 0.11372549, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.07058824, 0.36078431,\n",
              "       0.9372549 , 0.98823529, 0.98823529, 0.95294118, 0.25490196,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.81568627, 0.98823529,\n",
              "       0.98823529, 0.57647059, 0.5254902 , 0.5254902 , 0.5254902 ,\n",
              "       0.5254902 , 0.79607843, 0.99215686, 0.98823529, 0.98823529,\n",
              "       0.7372549 , 0.3254902 , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.81568627, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.99215686, 0.90196078, 0.6       , 0.03137255, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.19215686,\n",
              "       0.61568627, 0.98823529, 0.98823529, 0.98823529, 0.98823529,\n",
              "       0.98823529, 0.85098039, 0.81176471, 0.57254902, 0.17647059,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.02745098, 0.40392157,\n",
              "       0.92156863, 0.98823529, 0.6745098 , 0.40392157, 0.09411765,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFbxcwcENmQk"
      },
      "source": [
        "#2.1 Build Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uqi_S-ZPhn9"
      },
      "outputs": [],
      "source": [
        "def init_params():\n",
        "\n",
        "  #input layer's weigths\n",
        "  W1 = np.random.uniform(-0.5, 0.5, (16,784))\n",
        "  b1 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Hidden layer's weight\n",
        "  W2 = np.random.uniform(-0.5,0.5, (16,16))\n",
        "  b2 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "  # Second Hidden layer' weight\n",
        "  W3 = np.random.uniform(-0.5,0.5 ,(10,16))  # Weights are still to big\n",
        "  b3 = np.zeros((10,1))\n",
        "\n",
        "\n",
        "  return W1,b1,W2,b2,W3,b3\n",
        "\n",
        "\n",
        "\n",
        "W1,b1,W2,b2,W3,b3 = init_params()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRw2jsVWH0I4"
      },
      "source": [
        "#1.2 Build Activation functions and Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBsAF3drU98m",
        "outputId": "338feec9-0fb4-468a-b7bd-2c24981b3517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00679114 0.00527258 0.0658074  ... 0.00207419 0.00600942 0.03091759]\n",
            " [0.01790551 0.28869534 0.05381954 ... 0.18655483 0.14435255 0.18302996]\n",
            " [0.78716232 0.30405957 0.36980117 ... 0.50645333 0.02556571 0.22413396]\n",
            " ...\n",
            " [0.0109231  0.03390429 0.05191376 ... 0.04565613 0.09377314 0.06274774]\n",
            " [0.00242549 0.00778729 0.02485256 ... 0.00833378 0.00270395 0.09929324]\n",
            " [0.02089471 0.00800702 0.0968669  ... 0.00419734 0.00102753 0.06981688]]\n"
          ]
        }
      ],
      "source": [
        "def ReLU(Z):\n",
        "\n",
        "  return np.maximum(0,Z)\n",
        "\n",
        "\n",
        "def softmax(Z): # CHECK IT !!!\n",
        "\n",
        "  # for one examples is (np.exp(Z) / np.sum( np.exp(Z)))\n",
        "\n",
        "    Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
        "    sum_Z_exp = np.sum(Z_exp, axis=0)\n",
        "    softmax_output = Z_exp / sum_Z_exp\n",
        "\n",
        "    return softmax_output\n",
        "\n",
        "def sigmoid(Z):\n",
        "\n",
        "  return 1 / 1+ np.exp(-Z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def forward_prop(X,W1,b1,W2,b2,W3,b3):\n",
        "  #Forward propagation with 3 layered neural network\n",
        "\n",
        "  #Initialize  Z\n",
        "  #Since X is shape as (6000,784) and our W1 is shape as (16,784) in order to dot product we need to take Transpose of X\n",
        "\n",
        "  Z1 = W1.dot(X.T)+ b1\n",
        "  #Activation with ReLU to given linear regression to feed neural network\n",
        "  A1 = ReLU(Z1)\n",
        "\n",
        "  Z2 = W2.dot(A1) + b2\n",
        "\n",
        "\n",
        "  A2 = ReLU(Z2)\n",
        "\n",
        "  Z3 = W3.dot(A2) + b3\n",
        "  #At the end softmax  the output layer to have a probabilistic values\n",
        "  A3 = softmax(Z3)\n",
        "\n",
        "  return A1,A2,A3\n",
        "\n",
        "A1,A2,A3 = forward_prop(X_train,W1,b1,W2,b2,W3,b3)\n",
        "\n",
        "print(A3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFn_mUjHPKab"
      },
      "source": [
        "#1.3 Back-Propagation with One-hot and derivative of activaton functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dhSX3T_HAx9"
      },
      "outputs": [],
      "source": [
        "def one_hot(Y): # CHECK IT !!\n",
        "  num_classes = np.max(Y) + 1\n",
        "\n",
        "  one_hot = np.zeros((Y.size,num_classes))\n",
        "  one_hot[np.arange(Y.size), Y] = 1\n",
        "\n",
        "  return one_hot.T\n",
        "\n",
        "def derivative_ReLU(Z):\n",
        "  #Do we need to check between 0-1 ?\n",
        "  return Z > 0\n",
        "\n",
        "\n",
        "def backward_prop(X, Y, W1, b1, W2, b2, W3, b3, A1, A2, A3): # CHECK IT !!!!\n",
        "\n",
        "  m = Y.size\n",
        "  #One hot encoded to see each labels in matrix as 1\n",
        "  Y = one_hot(Y)\n",
        "\n",
        "  #Derivate of cost function with respect to z3\n",
        "  dZ3 = A3 - Y\n",
        "  dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "  dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
        "\n",
        "  #Derivate for second hidden layer\n",
        "  dA2 = np.dot(W3.T,dZ3)\n",
        "  dZ2 =  dA2  * derivative_ReLU(A2)\n",
        "  dW2 = (1/ m) * dZ2.dot(A1.T)\n",
        "  dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
        "\n",
        "  #Derivate for first hidden layer\n",
        "\n",
        "  dA1 = np.dot(W2.T,dZ2)\n",
        "  dZ1 = dA1 * derivative_ReLU(A1)\n",
        "  dW1 = (1/m) * dZ1.dot(X)\n",
        "  dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
        "\n",
        "  return dW1,dB1,dW2,dB2,dW3,dB3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tYISPvOQRQn"
      },
      "source": [
        "#1.3 Update the Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFbTDfoWQYq-"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "epochs =  1000\n",
        "\n",
        "def update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate):\n",
        "\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "\n",
        "    return W1,b1,W2,b2,W3,b3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhb1TpDrQtKt"
      },
      "source": [
        "#1.4 Try in given epochs time to see the changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-QTIprNdkBP"
      },
      "outputs": [],
      "source": [
        "def accuracy_score(A,Y):\n",
        "    size = Y.size\n",
        "    predict = np.argmax(A,0)\n",
        "\n",
        "    correct = np.sum(predict == Y)\n",
        "\n",
        "    accuracy = correct / size\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "#print(predictions.max)\n",
        "#print(y_train)\n",
        "#print(np.sum(y_train == predictions))\n",
        "#print(y_train.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLlbWaJGN7Pw",
        "outputId": "eecb592c-7f38-4726-8423-b3297eaa559c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy in 0 is 0.6655\n",
            "The accuracy in 20 is 0.7005333333333333\n",
            "The accuracy in 40 is 0.72825\n",
            "The accuracy in 60 is 0.7502833333333333\n",
            "The accuracy in 80 is 0.7692\n",
            "The accuracy in 100 is 0.7851666666666667\n",
            "The accuracy in 120 is 0.7992333333333334\n",
            "The accuracy in 140 is 0.81045\n",
            "The accuracy in 160 is 0.8194833333333333\n",
            "The accuracy in 180 is 0.8268333333333333\n",
            "The accuracy in 200 is 0.8329166666666666\n",
            "The accuracy in 220 is 0.8385333333333334\n",
            "The accuracy in 240 is 0.843\n",
            "The accuracy in 260 is 0.8265666666666667\n",
            "The accuracy in 280 is 0.8520166666666666\n",
            "The accuracy in 300 is 0.8557333333333333\n",
            "The accuracy in 320 is 0.8584833333333334\n",
            "The accuracy in 340 is 0.86105\n",
            "The accuracy in 360 is 0.86335\n",
            "The accuracy in 380 is 0.8658833333333333\n",
            "The accuracy in 400 is 0.86815\n",
            "The accuracy in 420 is 0.8708166666666667\n",
            "The accuracy in 440 is 0.8726833333333334\n",
            "The accuracy in 460 is 0.87495\n",
            "The accuracy in 480 is 0.87665\n",
            "The accuracy in 500 is 0.8784666666666666\n",
            "The accuracy in 520 is 0.8802833333333333\n",
            "The accuracy in 540 is 0.8818\n",
            "The accuracy in 560 is 0.8833666666666666\n",
            "The accuracy in 580 is 0.8850166666666667\n",
            "The accuracy in 600 is 0.8861333333333333\n",
            "The accuracy in 620 is 0.8873\n",
            "The accuracy in 640 is 0.8884166666666666\n",
            "The accuracy in 660 is 0.8898166666666667\n",
            "The accuracy in 680 is 0.8907666666666667\n",
            "The accuracy in 700 is 0.89175\n",
            "The accuracy in 720 is 0.8929666666666667\n",
            "The accuracy in 740 is 0.89385\n",
            "The accuracy in 760 is 0.8947166666666667\n",
            "The accuracy in 780 is 0.8958833333333334\n",
            "The accuracy in 800 is 0.8969666666666667\n",
            "The accuracy in 820 is 0.8978\n",
            "The accuracy in 840 is 0.8985333333333333\n",
            "The accuracy in 860 is 0.8991833333333333\n",
            "The accuracy in 880 is 0.9001333333333333\n",
            "The accuracy in 900 is 0.9009833333333334\n",
            "The accuracy in 920 is 0.90185\n",
            "The accuracy in 940 is 0.9025333333333333\n",
            "The accuracy in 960 is 0.9030666666666667\n",
            "The accuracy in 980 is 0.9037\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "   # Forward propagation\n",
        "   A1, A2, A3 = forward_prop(X_train, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "   #Backward propagation\n",
        "   dW1, db1, dW2, db2, dW3, db3 = backward_prop(X_train, y_train, W1, b1, W2, b2, W3, b3, A1, A2, A3)\n",
        "\n",
        "   #Updating gradients\n",
        "   W1,b1,W2,b2,W3,b3 = update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "   if epoch % 20 == 0:\n",
        "    acc =accuracy_score(A3,y_train)\n",
        "    print(f'The accuracy in {epoch} is {acc}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVjt2XwWdwhE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Razbolt/Neural-Network/blob/main/Neural_Network_Math_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGR6imnPESL"
      },
      "source": [
        "# 1.1 Install the MNIST and requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "j7joMCD_8U_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import load_digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9rb-9S-KJrD",
        "outputId": "67a98d93-63de-44df-afcf-2b20f8dfb497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-mnist\n",
            "  Using cached python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement quiet (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for quiet\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install python-mnist -- quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qc1NylZkLXAz"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "# Using keras libraries to take MNIST dataset\n",
        "(X_train, y_train,), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6uJCuvDsoP2"
      },
      "source": [
        "## 1.1 Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4kGn7-Ps25y"
      },
      "source": [
        "Before procesing, we need to check the MNIST Data.\n",
        "\n",
        "As we can see the X_train data is numbered from 0 to 255. In order to use them we need to normalize them as dividing to 255 each of the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ1--VdWCEzT",
        "outputId": "69f3a8c0-361f-42e4-ea3b-07d7a0d456a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UIxpHoosaud"
      },
      "source": [
        "We also need to check the shapes before feeding them into model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJs76_reOzPj",
        "outputId": "58d2aaa7-8d23-4874-ebe1-d9db9bec039c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zdUnLliLXu2",
        "outputId": "a875fade-abc2-4672-e997-f31a22e726a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9u7khFALZFa",
        "outputId": "c548320b-5844-426d-acd8-87af98bae5e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fHFqMAQBRrf",
        "outputId": "e16a193b-4b11-4f45-b77b-404151b3117e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784) (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Since we see the shapes are (6000,28,28) we need to reshape them to feed our model\n",
        "\n",
        "X_train = X_train.reshape(60000, 784) / 255.0\n",
        "\n",
        "X_test = X_test.reshape(10000, 784) /255.0\n",
        "\n",
        "print(X_train.shape , X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2e-p43TCRRW",
        "outputId": "00e7aa2a-5fd1-446f-ab04-35a963306807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
            " 0.96862745 0.49803922 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
            " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.19215686\n",
            " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
            " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
            " 0.96862745 0.94509804 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
            " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.04313725\n",
            " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.1372549  0.94509804\n",
            " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
            " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
            " 0.58823529 0.10588235 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
            " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15294118 0.58039216\n",
            " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.67058824\n",
            " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
            " 0.31372549 0.03529412 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.53333333 0.99215686\n",
            " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ],
      "source": [
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFbxcwcENmQk"
      },
      "source": [
        "# 2.1 Build Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6uqi_S-ZPhn9"
      },
      "outputs": [],
      "source": [
        "def init_params():\n",
        "\n",
        "  #input layer's weigths\n",
        "  W1 = np.random.uniform(-0.5, 0.5, (16,784))\n",
        "  b1 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Hidden layer's weight\n",
        "  W2 = np.random.uniform(-0.5,0.5, (16,16))\n",
        "  b2 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "  # Second Hidden layer' weight\n",
        "  W3 = np.random.uniform(-0.5,0.5 ,(10,16))  # Weights are still to big\n",
        "  b3 = np.zeros((10,1))\n",
        "\n",
        "\n",
        "  return W1,b1,W2,b2,W3,b3\n",
        "\n",
        "\n",
        "\n",
        "W1,b1,W2,b2,W3,b3 = init_params()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRw2jsVWH0I4"
      },
      "source": [
        "## 2.2 Build Activation functions and Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBsAF3drU98m",
        "outputId": "338feec9-0fb4-468a-b7bd-2c24981b3517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.06820694 0.04162403 0.07026699 ... 0.05978667 0.35221882 0.08964362]\n",
            " [0.01040265 0.0015661  0.01969245 ... 0.02353758 0.0017963  0.05099136]\n",
            " [0.06030815 0.01040692 0.02099445 ... 0.05829386 0.00595718 0.09804368]\n",
            " ...\n",
            " [0.00629022 0.00300928 0.01737721 ... 0.00858704 0.01455484 0.01946287]\n",
            " [0.03105714 0.09377505 0.03157601 ... 0.04281387 0.12934935 0.0952058 ]\n",
            " [0.00122951 0.00209431 0.00241041 ... 0.00212268 0.00427345 0.022361  ]]\n"
          ]
        }
      ],
      "source": [
        "def ReLU(Z):\n",
        "\n",
        "  return np.maximum(0,Z)\n",
        "\n",
        "\n",
        "def softmax(Z): # CHECK IT !!!\n",
        "\n",
        "  # for one examples is (np.exp(Z) / np.sum( np.exp(Z)))\n",
        "\n",
        "    Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
        "    sum_Z_exp = np.sum(Z_exp, axis=0)\n",
        "    softmax_output = Z_exp / sum_Z_exp\n",
        "\n",
        "    return softmax_output\n",
        "\n",
        "def sigmoid(Z):\n",
        "\n",
        "  return 1 / 1+ np.exp(-Z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def forward_prop(X,W1,b1,W2,b2,W3,b3,dropout = 0.0):\n",
        "  #We are assuming to have 3 layered neural network \n",
        "  #D1 and D2 are mask layers for dropout \n",
        "\n",
        "  D1= None\n",
        "  D2=None\n",
        "\n",
        "  # if there is no dropout layer\n",
        "  if (dropout) == 0.0:\n",
        "    #Forward propagation with 3 layered neural network\n",
        "\n",
        "    #Initialize  Z\n",
        "    #Since X is shape as (6000,784) and our W1 is shape as (16,784) in order to dot product we need to take Transpose of X\n",
        "\n",
        "    Z1 = W1.dot(X.T)+ b1\n",
        "    #Activation with ReLU to given linear regression to feed neural network\n",
        "    A1 = ReLU(Z1)\n",
        "\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "\n",
        "\n",
        "    A2 = ReLU(Z2)\n",
        "\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    #At the end softmax  the output layer to have a probabilistic values\n",
        "    A3 = softmax(Z3)\n",
        "\n",
        " \n",
        "\n",
        "  else: # If there is a droput layer, we are going to build mask for each layer\n",
        "    #Forward propagation with 3 layered neural network \n",
        "\n",
        "    Z1 = W1.dot(X.T)+ b1\n",
        "    #Activation with ReLU to given linear regression to feed neural network\n",
        "    A1 = ReLU(Z1)\n",
        "    '''\n",
        "    This line of code for dropout inspired from Andrew Ng's Deep Learning Specialization Course\n",
        "    https://www.youtube.com/watch?v=D8PJAL-MZv8\n",
        "    '''\n",
        "    #Create a mask for A1 as use probability of dropout from user\n",
        "    D1 = np.random.rand(*A1.shape) > dropout #D1 is mask matrix that check the proability of dropout.\n",
        "                                                          #If the probability is bigger than dropout, it will be 1 otherwise 0\n",
        "\n",
        "    #Apply mask to A1\n",
        "    A1 = A1 * D1\n",
        "    #Normalize A1 to not to change expected value of A1 \n",
        "    A1 = A1 / (1-dropout) # It scales A to not to change expected value of A1 as keeping  probability \n",
        "                          #keeeping probability means 1 - dropout probability\n",
        "\n",
        "   \n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    \n",
        "    #Create a mask for second hidden layer as use probability of dropout from user\n",
        "    D2 = np.random.rand( *A2.shape) > dropout\n",
        "    A2 = A2 * D2\n",
        "    A2 = A2 / (1-dropout)\n",
        "\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    #At the end softmax  the output layer to have a probabilistic values\n",
        "    A3 = softmax(Z3)\n",
        "\n",
        "  return A1,A2,A3,D1,D2\n",
        "\n",
        "A1,A2,A3,D1,D2 = forward_prop(X_train,W1,b1,W2,b2,W3,b3)\n",
        "\n",
        "print(A3)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60000\n"
          ]
        }
      ],
      "source": [
        "print(A1.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFn_mUjHPKab"
      },
      "source": [
        "## 2.3 Back-Propagation with One-hot and derivative of activaton functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5dhSX3T_HAx9"
      },
      "outputs": [],
      "source": [
        "def one_hot(Y): # CHECK IT !!\n",
        "  num_classes = np.max(Y) + 1\n",
        "\n",
        "  one_hot = np.zeros((Y.size,num_classes))\n",
        "  one_hot[np.arange(Y.size), Y] = 1\n",
        "\n",
        "  return one_hot.T\n",
        "\n",
        "def derivative_ReLU(Z):\n",
        "  #Do we need to check between 0-1 ?\n",
        "  return Z > 0\n",
        "\n",
        "#Focus on backpropagation !!! Check how its works ! \n",
        "def backward_prop(X, Y, W1, b1, W2, b2, W3, b3, A1, A2, A3,D1, D2, dropout = 0.0): # CHECK IT !!!!\n",
        "\n",
        "  m = Y.size\n",
        "  Y = one_hot(Y)\n",
        "  #if droput is not used\n",
        "\n",
        "  if (dropout) == 0.0:\n",
        "    \n",
        "    #One hot encoded to see each labels in matrix as 1\n",
        "    \n",
        "\n",
        "    #Derivate of cost function with respect to z3\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "    dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
        "\n",
        "    #Derivate for second hidden layer\n",
        "    dA2 = np.dot(W3.T,dZ3)\n",
        "    dZ2 =  dA2  * derivative_ReLU(A2)\n",
        "    dW2 = (1/ m) * dZ2.dot(A1.T)\n",
        "    dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
        "\n",
        "    #Derivate for first hidden layer\n",
        "\n",
        "    dA1 = np.dot(W2.T,dZ2)\n",
        "    dZ1 = dA1 * derivative_ReLU(A1)\n",
        "    dW1 = (1/m) * dZ1.dot(X)\n",
        "    dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
        "\n",
        "  else: #If droput is used\n",
        "\n",
        "    #Derivate of cost function with respect to z3\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "    dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
        "\n",
        "    #Derivate for second hidden layer\n",
        "    dA2 = np.dot(W3.T,dZ3)\n",
        "    #Dropout mask for second hidden layer\n",
        "    dA2 = dA2 * (D2)\n",
        "    #dA2 = D2 / dropout\n",
        "\n",
        "    dZ2 =  dA2  * derivative_ReLU(A2)\n",
        "    dW2 = (1/ m) * dZ2.dot(A1.T)\n",
        "    dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
        "\n",
        "    #Derivate for first hidden layer\n",
        "\n",
        "    dA1 = np.dot(W2.T,dZ2)\n",
        "    #Dropout mask for first hidden layer\n",
        "    dA1 = dA1 * (D1)\n",
        "    #dA1 = D1 / dropout\n",
        "\n",
        "    dZ1 = dA1 * derivative_ReLU(A1)\n",
        "    dW1 = (1/m) * dZ1.dot(X)\n",
        "    dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
        "    \n",
        "\n",
        "  return dW1,dB1,dW2,dB2,dW3,dB3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tYISPvOQRQn"
      },
      "source": [
        "## 2.4 Update the Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MFbTDfoWQYq-"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "epochs =  1500\n",
        "\n",
        "def update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate):\n",
        "\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "\n",
        "    return W1,b1,W2,b2,W3,b3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhb1TpDrQtKt"
      },
      "source": [
        "## 2.5 Try in given epochs time to see the changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "i-QTIprNdkBP"
      },
      "outputs": [],
      "source": [
        "def accuracy_score(A,Y):\n",
        "    size = Y.size\n",
        "    predict = np.argmax(A,0)\n",
        "\n",
        "    correct = np.sum(predict == Y)\n",
        "\n",
        "    accuracy = correct / size\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "#print(predictions.max)\n",
        "#print(y_train)\n",
        "#print(np.sum(y_train == predictions))\n",
        "#print(y_train.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLlbWaJGN7Pw",
        "outputId": "eecb592c-7f38-4726-8423-b3297eaa559c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy in epoch 0 is 0.055183333333333334\n",
            "The accuracy in epoch 100 is 0.4797666666666667\n",
            "The accuracy in epoch 200 is 0.59435\n",
            "The accuracy in epoch 300 is 0.64415\n",
            "The accuracy in epoch 400 is 0.6752166666666667\n",
            "The accuracy in epoch 500 is 0.70105\n"
          ]
        }
      ],
      "source": [
        "dropout = 0.1\n",
        "for epoch in range(epochs):\n",
        "   # Forward propagation\n",
        "   A1, A2, A3,D1,D2 = forward_prop(X_train, W1, b1, W2, b2, W3, b3,dropout=dropout)\n",
        "\n",
        "   #Backward propagation\n",
        "   dW1, db1, dW2, db2, dW3, db3 = backward_prop(X_train, y_train, W1, b1, W2, b2, W3, b3, A1, A2, A3,D1,D2,dropout=dropout)\n",
        "\n",
        "   #Updating gradients\n",
        "   W1,b1,W2,b2,W3,b3 = update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "   if epoch % 100 == 0:\n",
        "    acc =accuracy_score(A3,y_train)\n",
        "    print(f'The accuracy in epoch {epoch} is {acc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full Connected Class Version "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hey Greg Please check this part! \n",
        "\n",
        "I havent updated with comments because most of them written in above "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  Missing parts\n",
        "\n",
        "\n",
        "\n",
        "- part d)\n",
        "\n",
        "    1. update rule\n",
        "    2. decay\n",
        "    3. L1 OR L2 regularizator\n",
        "\n",
        "\n",
        "- part e) Optimizer\n",
        "\n",
        "    3. mini-batch gradient\n",
        "    4. stoacstic- graident descent \n",
        "\n",
        "May need to check how these gonna change the functions: train, forward and backward prop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "    def __init__(self,input_size,hidden_size,output_size,activation_function,dropout):\n",
        "        self.activation_function = activation_function\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.W1 = np.random.uniform(-0.5,0.5,(hidden_size,input_size))\n",
        "        self.b1 = np.zeros((hidden_size,1))\n",
        "\n",
        "        self.W2 = np.random.uniform(-0.5,0.5,(hidden_size,hidden_size))\n",
        "        self.b2 = np.zeros((hidden_size,1))\n",
        "\n",
        "        self.W3 = np.random.uniform(-0.5,0.5,(output_size,hidden_size))\n",
        "        self.b3 = np.zeros((output_size,1))\n",
        "    \n",
        "    #Staticmethod is used to call the function without creating an object\n",
        "    #In this way we can call them in the activation function and deactivation function\n",
        "    @staticmethod  \n",
        "    def ReLU(Z):\n",
        "        return np.maximum(0,Z)\n",
        "    \n",
        "    @staticmethod\n",
        "    def derivative_ReLU(Z):\n",
        "        return Z > 0\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(Z):\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        "    \n",
        "    @staticmethod\n",
        "    def derivative_sigmoid(Z):\n",
        "        return MyNeuralNetwork.sigmoid(Z) * (1 - MyNeuralNetwork.sigmoid(Z))\n",
        "    \n",
        "    @staticmethod\n",
        "    def softmax(Z):\n",
        "        Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
        "        sum_Z_exp = np.sum(Z_exp, axis=0)\n",
        "        softmax_output = Z_exp / sum_Z_exp\n",
        "        return softmax_output\n",
        "    \n",
        "\n",
        "    \n",
        "    def one_hot(self,Y):\n",
        "        num_classes = np.max(Y) + 1\n",
        "        one_hot = np.zeros((Y.size,num_classes))\n",
        "        one_hot[np.arange(Y.size), Y] = 1\n",
        "        return one_hot.T\n",
        "    \n",
        "    \n",
        "    def activation(self,Z):\n",
        "        if self.activation_function == 'relu':\n",
        "            return self.ReLU(Z)\n",
        "        elif self.activation_function == 'sigmoid':\n",
        "            return self.sigmoid(Z)\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return self.softmax(Z)\n",
        "        else:\n",
        "            raise Exception('Activation function not supported')\n",
        "        \n",
        "    def derivative_activation(self,A):\n",
        "        if self.activation_function == 'relu':\n",
        "            return self.derivative_ReLU(A)\n",
        "        elif self.activation_function == 'sigmoid':\n",
        "            return self.derivative_sigmoid(A)\n",
        "        else:\n",
        "            raise Exception('Activation function not supported')\n",
        "    \n",
        "    def forward_prop(self,X,dropout):\n",
        "        D1 = None\n",
        "        D2 = None\n",
        "        #Forward propagation with 3 layered neural network\n",
        "        \n",
        "        if dropout == 0.0: # If there is no dropout layer\n",
        "            Z1 = self.W1.dot(X.T) + self.b1\n",
        "            A1 = self.activation(Z1)\n",
        "\n",
        "            Z2 = self.W2.dot(A1) + self.b2\n",
        "            A2 = self.activation(Z2)\n",
        "\n",
        "            Z3 = self.W3.dot(A2) + self.b3\n",
        "            A3 = self.softmax(Z3)\n",
        "        else:              # If there is a dropout layer \n",
        "            \"\"\" This line of code for dropout inspired from Andrew Ng's Deep Learning Specialization Course\"\"\"\n",
        "            \"\"\" https://www.youtube.com/watch?v=D8PJAL-MZv8 \"\"\"\n",
        "\n",
        "            # Create a mask for A1 as use probability of dropout from user\n",
        "            Z1 = self.W1.dot(X.T) + self.b1 \n",
        "            A1 = self.activation(Z1)\n",
        "    \n",
        "            D1 = np.random.rand(*A1.shape) > dropout # D1 is mask matrix that check the proability of dropout.\n",
        "            A1 = A1 * D1                           # If the probability is bigger than dropout, it will be 1 otherwise 0\n",
        "            A1 = A1 / (1 - dropout)                # It scales A to not to change expected value of A1 as keeping  probability\n",
        "\n",
        "            Z2 = self.W2.dot(A1) + self.b2\n",
        "            A2 = self.activation(Z2)\n",
        "\n",
        "            D2 = np.random.rand(*A2.shape) > dropout\n",
        "            A2 = A2 * D2\n",
        "            A2 = A2 / (1 - dropout)\n",
        "\n",
        "            Z3 = self.W3.dot(A2) + self.b3 \n",
        "            A3 = self.softmax(Z3) # At the end softmax  the output layer to have a probabilistic values\n",
        "        \n",
        "        return A1,A2,A3,D1,D2\n",
        "    \n",
        "    def backward_prop(self,X,Y,A1,A2,A3,D1,D2,dropout):\n",
        "        m = Y.size\n",
        "        Y = self.one_hot(Y)\n",
        "        \n",
        "        # Derivate of cost function with respect to z3\n",
        "        dZ3 = A3 - Y\n",
        "        dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "        db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "        # Derivate for second hidden layer\n",
        "        dA2 = np.dot(self.W3.T, dZ3)\n",
        "        if dropout > 0:\n",
        "            dA2 = dA2 * D2  # Apply dropout\n",
        "        dZ2 = dA2 * self.derivative_activation(A2)\n",
        "        dW2 = (1/m) * dZ2.dot(A1.T)\n",
        "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "        # Derivate for first hidden layer\n",
        "        dA1 = np.dot(self.W2.T, dZ2)\n",
        "        if dropout > 0:\n",
        "            dA1 = dA1 * D1  # Apply dropout\n",
        "        dZ1 = dA1 * self.derivative_activation(A1)\n",
        "        dW1 = (1/m) * dZ1.dot(X)\n",
        "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "        # Store the gradients\n",
        "        self.dW1 = dW1\n",
        "        self.db1 = db1\n",
        "        self.dW2 = dW2\n",
        "        self.db2 = db2\n",
        "        self.dW3 = dW3\n",
        "        self.db3 = db3\n",
        "\n",
        "    def update_gradient(self,learning_rate):\n",
        "        self.W1 -= learning_rate * self.dW1\n",
        "        self.b1 -= learning_rate * self.db1\n",
        "        self.W2 -= learning_rate * self.dW2\n",
        "        self.b2 -= learning_rate * self.db2\n",
        "        self.W3 -= learning_rate * self.dW3\n",
        "        self.b3 -= learning_rate * self.db3\n",
        "\n",
        "\n",
        "    def train(self,X,Y,learning_rate,epochs): # Need some updates based on batch size, mini-batch  etc.\n",
        "        for epoch in range(epochs):\n",
        "            # Forward propagation\n",
        "            A1, A2, A3, D1, D2 = self.forward_prop(X, self.dropout)\n",
        "\n",
        "            # Backward propagation\n",
        "            self.backward_prop(X, Y, A1, A2, A3, D1, D2, self.dropout)\n",
        "\n",
        "            # Updating gradients\n",
        "            self.update_gradient(learning_rate)\n",
        "\n",
        "\n",
        "    def predict(self,X):\n",
        "        A1, A2, A3, D1, D2 = self.forward_prop(X, dropout=0.0)\n",
        "        return np.argmax(A3, axis=0)\n",
        "\n",
        "    def accuracy_score(self,X,Y):\n",
        "        size = Y.size\n",
        "        predict = self.predict(X)\n",
        "        correct = np.sum(predict == Y)\n",
        "        accuracy = correct / size\n",
        "        return accuracy\n",
        "\n",
        "    def test(self,X,Y):\n",
        "        accuracy = self.accuracy_score(X,Y)\n",
        "        print(f'The accuracy is {accuracy*100}')\n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn = MyNeuralNetwork(input_size=784, hidden_size=30, output_size=10, activation_function='relu',dropout=0.2)\n",
        "\n",
        "nn.train(X_train, y_train, learning_rate=0.2, epochs=200)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy is 78.49000000000001\n"
          ]
        }
      ],
      "source": [
        "nn.test(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNLCAXIYYGmpU8xJwnPWCXF",
      "collapsed_sections": [
        "_WGR6imnPESL",
        "aFbxcwcENmQk",
        "IRw2jsVWH0I4"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

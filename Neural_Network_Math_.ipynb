{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Razbolt/Neural-Network/blob/main/Neural_Network_Math_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGR6imnPESL"
      },
      "source": [
        "# 1.1 Install the MNIST and requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j7joMCD_8U_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import load_digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9rb-9S-KJrD",
        "outputId": "67a98d93-63de-44df-afcf-2b20f8dfb497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-mnist\n",
            "  Using cached python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement quiet (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for quiet\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install python-mnist -- quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qc1NylZkLXAz"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "# Using keras libraries to take MNIST dataset\n",
        "(X_train, y_train,), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6uJCuvDsoP2"
      },
      "source": [
        "## 1.1 Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4kGn7-Ps25y"
      },
      "source": [
        "Before procesing, we need to check the MNIST Data.\n",
        "\n",
        "As we can see the X_train data is numbered from 0 to 255. In order to use them we need to normalize them as dividing to 255 each of the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ1--VdWCEzT",
        "outputId": "69f3a8c0-361f-42e4-ea3b-07d7a0d456a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UIxpHoosaud"
      },
      "source": [
        "We also need to check the shapes before feeding them into model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJs76_reOzPj",
        "outputId": "58d2aaa7-8d23-4874-ebe1-d9db9bec039c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zdUnLliLXu2",
        "outputId": "a875fade-abc2-4672-e997-f31a22e726a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9u7khFALZFa",
        "outputId": "c548320b-5844-426d-acd8-87af98bae5e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fHFqMAQBRrf",
        "outputId": "e16a193b-4b11-4f45-b77b-404151b3117e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784) (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Since we see the shapes are (6000,28,28) we need to reshape them to feed our model\n",
        "\n",
        "X_train = X_train.reshape(60000, 784) / 255.0\n",
        "\n",
        "X_test = X_test.reshape(10000, 784) /255.0\n",
        "\n",
        "print(X_train.shape , X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2e-p43TCRRW",
        "outputId": "00e7aa2a-5fd1-446f-ab04-35a963306807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFbxcwcENmQk"
      },
      "source": [
        "# 2.1 Build Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6uqi_S-ZPhn9"
      },
      "outputs": [],
      "source": [
        "def init_params():\n",
        "\n",
        "  #input layer's weigths\n",
        "  W1 = np.random.uniform(-0.5, 0.5, (16,784))\n",
        "  b1 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Hidden layer's weight\n",
        "  W2 = np.random.uniform(-0.5,0.5, (16,16))\n",
        "  b2 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "  # Second Hidden layer' weight\n",
        "  W3 = np.random.uniform(-0.5,0.5 ,(10,16))  # Weights are still to big\n",
        "  b3 = np.zeros((10,1))\n",
        "\n",
        "\n",
        "  return W1,b1,W2,b2,W3,b3\n",
        "\n",
        "\n",
        "\n",
        "W1,b1,W2,b2,W3,b3 = init_params()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRw2jsVWH0I4"
      },
      "source": [
        "## 2.2 Build Activation functions and Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBsAF3drU98m",
        "outputId": "338feec9-0fb4-468a-b7bd-2c24981b3517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.01500404 0.05363104 0.04667131 ... 0.0143118  0.02702216 0.01056243]\n",
            " [0.15155112 0.11413338 0.17878081 ... 0.27793059 0.09523603 0.29578992]\n",
            " [0.04291546 0.1660734  0.1049816  ... 0.05985431 0.10995541 0.06958951]\n",
            " ...\n",
            " [0.00810561 0.05842213 0.06729381 ... 0.02289159 0.0573601  0.02675677]\n",
            " [0.00293193 0.05473835 0.04720537 ... 0.007879   0.05444545 0.01719433]\n",
            " [0.48170369 0.12393148 0.1797272  ... 0.30842978 0.10535148 0.23981536]]\n"
          ]
        }
      ],
      "source": [
        "def ReLU(Z):\n",
        "\n",
        "  return np.maximum(0,Z)\n",
        "\n",
        "\n",
        "def softmax(Z): # CHECK IT !!!\n",
        "\n",
        "  # for one examples is (np.exp(Z) / np.sum( np.exp(Z)))\n",
        "\n",
        "    Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
        "    sum_Z_exp = np.sum(Z_exp, axis=0)\n",
        "    softmax_output = Z_exp / sum_Z_exp\n",
        "\n",
        "    return softmax_output\n",
        "\n",
        "def sigmoid(Z):\n",
        "\n",
        "  return 1 / 1+ np.exp(-Z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def forward_prop(X,W1,b1,W2,b2,W3,b3,dropout = 0.0):\n",
        "  #We are assuming to have 3 layered neural network \n",
        "  #D1 and D2 are mask layers for dropout \n",
        "\n",
        "  D1= None\n",
        "  D2=None\n",
        "\n",
        "  # if there is no dropout layer\n",
        "  if (dropout) == 0.0:\n",
        "    #Forward propagation with 3 layered neural network\n",
        "\n",
        "    #Initialize  Z\n",
        "    #Since X is shape as (6000,784) and our W1 is shape as (16,784) in order to dot product we need to take Transpose of X\n",
        "\n",
        "    Z1 = W1.dot(X.T)+ b1\n",
        "    #Activation with ReLU to given linear regression to feed neural network\n",
        "    A1 = ReLU(Z1)\n",
        "\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "\n",
        "\n",
        "    A2 = ReLU(Z2)\n",
        "\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    #At the end softmax  the output layer to have a probabilistic values\n",
        "    A3 = softmax(Z3)\n",
        "\n",
        " \n",
        "\n",
        "  else: # If there is a droput layer, we are going to build mask for each layer\n",
        "    #Forward propagation with 3 layered neural network \n",
        "\n",
        "    Z1 = W1.dot(X.T)+ b1\n",
        "    #Activation with ReLU to given linear regression to feed neural network\n",
        "    A1 = ReLU(Z1)\n",
        "    '''\n",
        "    This line of code for dropout inspired from Andrew Ng's Deep Learning Specialization Course\n",
        "    https://www.youtube.com/watch?v=D8PJAL-MZv8\n",
        "    '''\n",
        "    #Create a mask for A1 as use probability of dropout from user\n",
        "    D1 = np.random.rand(*A1.shape) > dropout #D1 is mask matrix that check the proability of dropout.\n",
        "                                                          #If the probability is bigger than dropout, it will be 1 otherwise 0\n",
        "\n",
        "    #Apply mask to A1\n",
        "    A1 = A1 * D1\n",
        "    #Normalize A1 to not to change expected value of A1 \n",
        "    A1 = A1 / (1-dropout) # It scales A to not to change expected value of A1 as keeping  probability \n",
        "                          #keeeping probability means 1 - dropout probability\n",
        "\n",
        "   \n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    \n",
        "    #Create a mask for second hidden layer as use probability of dropout from user\n",
        "    D2 = np.random.rand( *A2.shape) > dropout\n",
        "    A2 = A2 * D2\n",
        "    A2 = A2 / (1-dropout)\n",
        "\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    #At the end softmax  the output layer to have a probabilistic values\n",
        "    A3 = softmax(Z3)\n",
        "\n",
        "  return A1,A2,A3,D1,D2\n",
        "\n",
        "A1,A2,A3,D1,D2 = forward_prop(X_train,W1,b1,W2,b2,W3,b3)\n",
        "\n",
        "print(A3)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60000\n"
          ]
        }
      ],
      "source": [
        "print(A1.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFn_mUjHPKab"
      },
      "source": [
        "## 2.3 Back-Propagation with One-hot and derivative of activaton functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5dhSX3T_HAx9"
      },
      "outputs": [],
      "source": [
        "def one_hot(Y): # CHECK IT !!\n",
        "  num_classes = np.max(Y) + 1\n",
        "\n",
        "  one_hot = np.zeros((Y.size,num_classes))\n",
        "  one_hot[np.arange(Y.size), Y] = 1\n",
        "\n",
        "  return one_hot.T\n",
        "\n",
        "def derivative_ReLU(Z):\n",
        "  #Do we need to check between 0-1 ?\n",
        "  return Z > 0\n",
        "\n",
        "#Focus on backpropagation !!! Check how its works ! \n",
        "def backward_prop(X, Y, W1, b1, W2, b2, W3, b3, A1, A2, A3,D1, D2, dropout = 0.0): # CHECK IT !!!!\n",
        "\n",
        "  m = Y.size\n",
        "  Y = one_hot(Y)\n",
        "  #if droput is not used\n",
        "\n",
        "  if (dropout) == 0.0:\n",
        "    \n",
        "    #One hot encoded to see each labels in matrix as 1\n",
        "    \n",
        "\n",
        "    #Derivate of cost function with respect to z3\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "    dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
        "\n",
        "    #Derivate for second hidden layer\n",
        "    dA2 = np.dot(W3.T,dZ3)\n",
        "    dZ2 =  dA2  * derivative_ReLU(A2)\n",
        "    dW2 = (1/ m) * dZ2.dot(A1.T)\n",
        "    dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
        "\n",
        "    #Derivate for first hidden layer\n",
        "\n",
        "    dA1 = np.dot(W2.T,dZ2)\n",
        "    dZ1 = dA1 * derivative_ReLU(A1)\n",
        "    dW1 = (1/m) * dZ1.dot(X)\n",
        "    dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
        "\n",
        "  else: #If droput is used\n",
        "\n",
        "    #Derivate of cost function with respect to z3\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "    dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
        "\n",
        "    #Derivate for second hidden layer\n",
        "    dA2 = np.dot(W3.T,dZ3)\n",
        "    #Dropout mask for second hidden layer\n",
        "    dA2 = dA2 * (D2)\n",
        "    #dA2 = D2 / dropout\n",
        "\n",
        "    dZ2 =  dA2  * derivative_ReLU(A2)\n",
        "    dW2 = (1/ m) * dZ2.dot(A1.T)\n",
        "    dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
        "\n",
        "    #Derivate for first hidden layer\n",
        "\n",
        "    dA1 = np.dot(W2.T,dZ2)\n",
        "    #Dropout mask for first hidden layer\n",
        "    dA1 = dA1 * (D1)\n",
        "    #dA1 = D1 / dropout\n",
        "\n",
        "    dZ1 = dA1 * derivative_ReLU(A1)\n",
        "    dW1 = (1/m) * dZ1.dot(X)\n",
        "    dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
        "    \n",
        "\n",
        "  return dW1,dB1,dW2,dB2,dW3,dB3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tYISPvOQRQn"
      },
      "source": [
        "## 2.4 Update the Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MFbTDfoWQYq-"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "epochs =  200\n",
        "\n",
        "def update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate):\n",
        "\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "\n",
        "    return W1,b1,W2,b2,W3,b3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhb1TpDrQtKt"
      },
      "source": [
        "## 2.5 Try in given epochs time to see the changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "i-QTIprNdkBP"
      },
      "outputs": [],
      "source": [
        "def accuracy_score(A,Y):\n",
        "    size = Y.size\n",
        "    predict = np.argmax(A,0)\n",
        "\n",
        "    correct = np.sum(predict == Y)\n",
        "\n",
        "    accuracy = correct / size\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "#print(predictions.max)\n",
        "#print(y_train)\n",
        "#print(np.sum(y_train == predictions))\n",
        "#print(y_train.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLlbWaJGN7Pw",
        "outputId": "eecb592c-7f38-4726-8423-b3297eaa559c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy in epoch 0 is 0.10631666666666667\n",
            "The accuracy in epoch 100 is 0.4109333333333333\n"
          ]
        }
      ],
      "source": [
        "dropout = 0.1\n",
        "for epoch in range(epochs):\n",
        "   # Forward propagation\n",
        "   A1, A2, A3,D1,D2 = forward_prop(X_train, W1, b1, W2, b2, W3, b3,dropout=dropout)\n",
        "\n",
        "   #Backward propagation\n",
        "   dW1, db1, dW2, db2, dW3, db3 = backward_prop(X_train, y_train, W1, b1, W2, b2, W3, b3, A1, A2, A3,D1,D2,dropout=dropout)\n",
        "\n",
        "   #Updating gradients\n",
        "   W1,b1,W2,b2,W3,b3 = update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "   if epoch % 100 == 0:\n",
        "    acc =accuracy_score(A3,y_train)\n",
        "    print(f'The accuracy in epoch {epoch} is {acc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full Connected Class Version "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hey Greg Please check this part! \n",
        "\n",
        "I havent updated with comments because most of them written in above "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  Missing parts\n",
        "\n",
        "\n",
        "\n",
        "- part d)\n",
        "\n",
        "    1. update rule\n",
        "    2. decay\n",
        "    3. L1 OR L2 regularizator\n",
        "\n",
        "\n",
        "- part e) Optimizer\n",
        "\n",
        "    3. mini-batch gradient\n",
        "    4. stoacstic- graident descent \n",
        "\n",
        "May need to check how these gonna change the functions: train, forward and backward prop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "    def __init__(self,input_size,hidden_size,output_size,activation_function,dropout):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.activation_function = activation_function\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.W1 = np.random.uniform(-0.5,0.5,(hidden_size,input_size))\n",
        "        self.b1 = np.zeros((hidden_size,1))\n",
        "\n",
        "        self.W2 = np.random.uniform(-0.5,0.5,(hidden_size,hidden_size))\n",
        "        self.b2 = np.zeros((hidden_size,1))\n",
        "\n",
        "        self.W3 = np.random.uniform(-0.5,0.5,(output_size,hidden_size))\n",
        "        self.b3 = np.zeros((output_size,1))\n",
        "    \n",
        "    #Staticmethod is used to call the function without creating an object\n",
        "    #In this way we can call them in the activation function and deactivation function\n",
        "    @staticmethod  \n",
        "    def ReLU(Z):\n",
        "        return np.maximum(0,Z)\n",
        "    \n",
        "    @staticmethod\n",
        "    def derivative_ReLU(Z):\n",
        "        return Z > 0\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(Z):\n",
        "        # Cap the values of Z within the range [-700, 700], to prevent overflow\n",
        "        Z = np.clip(Z, -700, 700)\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        "    \n",
        "    @staticmethod\n",
        "    def derivative_sigmoid(Z):\n",
        "        return MyNeuralNetwork.sigmoid(Z) * (1 - MyNeuralNetwork.sigmoid(Z))\n",
        "    \n",
        "    @staticmethod\n",
        "    def softmax(Z):\n",
        "        Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
        "        sum_Z_exp = np.sum(Z_exp, axis=0)\n",
        "        softmax_output = Z_exp / sum_Z_exp\n",
        "        return softmax_output\n",
        "    \n",
        "\n",
        "    \n",
        "    def one_hot(self,Y):\n",
        "        num_classes = self.output_size\n",
        "        one_hot = np.zeros((Y.size,num_classes))\n",
        "        one_hot[np.arange(Y.size), Y] = 1\n",
        "        return one_hot.T\n",
        "    \n",
        "    \n",
        "    def activation(self,Z):\n",
        "        if self.activation_function == 'relu':\n",
        "            return self.ReLU(Z)\n",
        "        elif self.activation_function == 'sigmoid':\n",
        "            return self.sigmoid(Z)\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return self.softmax(Z)\n",
        "        else:\n",
        "            raise Exception('Activation function not supported')\n",
        "        \n",
        "    def derivative_activation(self,A):\n",
        "        if self.activation_function == 'relu':\n",
        "            return self.derivative_ReLU(A)\n",
        "        elif self.activation_function == 'sigmoid':\n",
        "            return self.derivative_sigmoid(A)\n",
        "        else:\n",
        "            raise Exception('Activation function not supported')\n",
        "        \n",
        "    def calculate_loss(self,A3,Y):\n",
        "        # Calculate the loss using the cross-entropy loss function\n",
        "        #Calcuation should be based on y_pred and y_true\n",
        "\n",
        "        y_pred = A3\n",
        "        y_true = self.one_hot(Y)\n",
        "\n",
        "        #Clip the y_prediction between epsilon and 1 - epsilon to prevent log(0) error\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "\n",
        "        #Calculate the loss\n",
        "        loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "        loss = np.sum(loss) / Y.size\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def forward_prop(self,X,dropout):\n",
        "        D1 = None\n",
        "        D2 = None\n",
        "        #Forward propagation with 3 layered neural network\n",
        "        \n",
        "        if dropout == 0.0: # If there is no dropout layer\n",
        "            Z1 = self.W1.dot(X.T) + self.b1\n",
        "            A1 = self.activation(Z1)\n",
        "\n",
        "            Z2 = self.W2.dot(A1) + self.b2\n",
        "            A2 = self.activation(Z2)\n",
        "\n",
        "            Z3 = self.W3.dot(A2) + self.b3\n",
        "            A3 = self.softmax(Z3)\n",
        "        else:              # If there is a dropout layer \n",
        "            \"\"\" This line of code for dropout inspired from Andrew Ng's Deep Learning Specialization Course\"\"\"\n",
        "            \"\"\" https://www.youtube.com/watch?v=D8PJAL-MZv8 \"\"\"\n",
        "\n",
        "            # Create a mask for A1 as use probability of dropout from user\n",
        "            Z1 = self.W1.dot(X.T) + self.b1 \n",
        "            A1 = self.activation(Z1)\n",
        "    \n",
        "            D1 = np.random.rand(*A1.shape) > dropout # D1 is mask matrix that check the proability of dropout.\n",
        "            A1 = A1 * D1                           # If the probability is bigger than dropout, it will be 1 otherwise 0\n",
        "            A1 = A1 / (1 - dropout)                # It scales A to not to change expected value of A1 as keeping  probability\n",
        "\n",
        "            Z2 = self.W2.dot(A1) + self.b2\n",
        "            A2 = self.activation(Z2)\n",
        "\n",
        "            D2 = np.random.rand(*A2.shape) > dropout\n",
        "            A2 = A2 * D2\n",
        "            A2 = A2 / (1 - dropout)\n",
        "\n",
        "            Z3 = self.W3.dot(A2) + self.b3 \n",
        "            A3 = self.softmax(Z3) # At the end softmax  the output layer to have a probabilistic values\n",
        "        \n",
        "        return A1,A2,A3,D1,D2\n",
        "    \n",
        "    def backward_prop(self,X,Y,A1,A2,A3,D1,D2,dropout):\n",
        "        m = Y.size\n",
        "        Y = self.one_hot(Y)\n",
        "        \n",
        "        # Derivate of cost function with respect to z3\n",
        "        dZ3 = A3 - Y\n",
        "        dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "        db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "        # Derivate for second hidden layer\n",
        "        dA2 = np.dot(self.W3.T, dZ3)\n",
        "        if dropout > 0:\n",
        "            dA2 = dA2 * D2  # Apply dropout\n",
        "        dZ2 = dA2 * self.derivative_activation(A2)\n",
        "        dW2 = (1/m) * dZ2.dot(A1.T)\n",
        "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "        # Derivate for first hidden layer\n",
        "        dA1 = np.dot(self.W2.T, dZ2)\n",
        "        if dropout > 0:\n",
        "            dA1 = dA1 * D1  # Apply dropout\n",
        "        dZ1 = dA1 * self.derivative_activation(A1)\n",
        "        dW1 = (1/m) * dZ1.dot(X)\n",
        "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "        # Store the gradients\n",
        "        self.dW1 = dW1\n",
        "        self.db1 = db1\n",
        "        self.dW2 = dW2\n",
        "        self.db2 = db2\n",
        "        self.dW3 = dW3\n",
        "        self.db3 = db3\n",
        "\n",
        "    def update_gradient(self,learning_rate):\n",
        "        self.W1 -= learning_rate * self.dW1\n",
        "        self.b1 -= learning_rate * self.db1\n",
        "        self.W2 -= learning_rate * self.dW2\n",
        "        self.b2 -= learning_rate * self.db2\n",
        "        self.W3 -= learning_rate * self.dW3\n",
        "        self.b3 -= learning_rate * self.db3\n",
        "\n",
        "\n",
        "    def train(self,X,Y,learning_rate,epochs,batch_size): \n",
        "        \"\"\"\n",
        "        Trains the neural network using the given training data.\n",
        "\n",
        "        Parameters:\n",
        "        self : MyNeuralNetwork\n",
        "            The neural network object to train.\n",
        "            \n",
        "\n",
        "        X = numpy.ndarray\n",
        "            The input data, wehere each row is a training example and each column is a feature.\n",
        "\n",
        "        Y = numpy.ndarray\n",
        "            The labels for each training example, where each row is a label. Must have the same number of rows as X.\n",
        "\n",
        "        learning_rate : float\n",
        "            The learning rate to use for weight updates in gradient descent.\n",
        "        epochs : int\n",
        "            The number of times to iterate over the entire training set.\n",
        "\n",
        "        batch_size : int\n",
        "            The number of training examples to split the training set into for mini-batch gradient descent.\n",
        "        \n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        losses = [] # Array to store the loss at each epoch\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            #Shuffle the dataset at the start of each epoc\n",
        "            permutation = np.random.permutation(m)\n",
        "            X_shuffled = X[permutation]\n",
        "            Y_shuffled = Y[permutation]\n",
        "\n",
        "            #Divide the the dataset into mini-bathces  \n",
        "            for i in range(0,m - (m % batch_size),batch_size):\n",
        "                X_batch = X_shuffled[i:i+batch_size]\n",
        "                Y_batch = Y_shuffled[i:i+batch_size]\n",
        "\n",
        "                # Forward propagation\n",
        "                A1, A2, A3, D1, D2 = self.forward_prop(X_batch, self.dropout)\n",
        "\n",
        "                # Calculate the loss and store it\n",
        "                loss = self.calculate_loss(A3, Y_batch)\n",
        "                losses.append(loss)\n",
        "\n",
        "\n",
        "                # Backward propagation\n",
        "                self.backward_prop(X_batch, Y_batch, A1, A2, A3, D1, D2, self.dropout)\n",
        "\n",
        "                # Updating gradients\n",
        "                self.update_gradient(learning_rate)\n",
        "            #Print the loss at each epoch with decimal point 4\n",
        "            print(f'The loss in epoch {epoch} is : {loss}')\n",
        "        return losses\n",
        "            \n",
        "\n",
        "    def predict(self,X):\n",
        "        A1, A2, A3, D1, D2 = self.forward_prop(X, dropout=0.0)\n",
        "        return np.argmax(A3, axis=0)\n",
        "\n",
        "    def accuracy_score(self,X,Y):\n",
        "        size = Y.size\n",
        "        predict = self.predict(X)\n",
        "        correct = np.sum(predict == Y)\n",
        "        accuracy = correct / size\n",
        "        return accuracy\n",
        "\n",
        "    def test(self,X,Y):\n",
        "        accuracy = self.accuracy_score(X,Y)\n",
        "        print(f'The accuracy is: {accuracy*100}')\n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The loss in epoch 0 is 2.2976340131158013\n",
            "The loss in epoch 1 is 2.0250404402798736\n",
            "The loss in epoch 2 is 1.8330039110375398\n",
            "The loss in epoch 3 is 1.518812696500279\n",
            "The loss in epoch 4 is 1.5669496750007186\n",
            "The loss in epoch 5 is 1.4342687819680162\n",
            "The loss in epoch 6 is 1.4972590854542174\n",
            "The loss in epoch 7 is 1.3518970657441969\n",
            "The loss in epoch 8 is 1.1875793081156463\n",
            "The loss in epoch 9 is 1.2054570877856143\n",
            "The loss in epoch 10 is 1.0123309664152096\n",
            "The loss in epoch 11 is 1.0536979932274495\n",
            "The loss in epoch 12 is 0.938402268891581\n",
            "The loss in epoch 13 is 1.055957154647075\n",
            "The loss in epoch 14 is 1.1639103802535826\n",
            "The loss in epoch 15 is 1.24750965031481\n",
            "The loss in epoch 16 is 1.0965713825306969\n",
            "The loss in epoch 17 is 0.8555921187159093\n",
            "The loss in epoch 18 is 0.970101953146548\n",
            "The loss in epoch 19 is 1.062227606330101\n"
          ]
        }
      ],
      "source": [
        "# Create a neural network object\n",
        "nn = MyNeuralNetwork(input_size=784, hidden_size=35, output_size=10, activation_function='relu',dropout=0.4)\n",
        "\n",
        "# Train the neural network\n",
        "losses = nn.train(X_train, y_train, learning_rate=0.1, epochs=50, batch_size=128)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy is 92.95\n"
          ]
        }
      ],
      "source": [
        "# Test the neural network\n",
        "nn.test(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNLCAXIYYGmpU8xJwnPWCXF",
      "collapsed_sections": [
        "_WGR6imnPESL",
        "aFbxcwcENmQk",
        "IRw2jsVWH0I4"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

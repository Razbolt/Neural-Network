{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Razbolt/Neural-Network/blob/main/Neural_Network_Math_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGR6imnPESL"
      },
      "source": [
        "# 1.1 Install the MNIST and requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j7joMCD_8U_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import load_digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9rb-9S-KJrD",
        "outputId": "67a98d93-63de-44df-afcf-2b20f8dfb497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-mnist\n",
            "  Using cached python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement quiet (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for quiet\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install python-mnist -- quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qc1NylZkLXAz"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "# Using keras libraries to take MNIST dataset\n",
        "(X_train, y_train,), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6uJCuvDsoP2"
      },
      "source": [
        "## 1.1 Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4kGn7-Ps25y"
      },
      "source": [
        "Before procesing, we need to check the MNIST Data.\n",
        "\n",
        "As we can see the X_train data is numbered from 0 to 255. In order to use them we need to normalize them as dividing to 255 each of the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ1--VdWCEzT",
        "outputId": "69f3a8c0-361f-42e4-ea3b-07d7a0d456a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UIxpHoosaud"
      },
      "source": [
        "We also need to check the shapes before feeding them into model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJs76_reOzPj",
        "outputId": "58d2aaa7-8d23-4874-ebe1-d9db9bec039c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zdUnLliLXu2",
        "outputId": "a875fade-abc2-4672-e997-f31a22e726a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9u7khFALZFa",
        "outputId": "c548320b-5844-426d-acd8-87af98bae5e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fHFqMAQBRrf",
        "outputId": "e16a193b-4b11-4f45-b77b-404151b3117e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784) (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Since we see the shapes are (6000,28,28) we need to reshape them to feed our model\n",
        "\n",
        "X_train = X_train.reshape(60000, 784) / 255.0\n",
        "\n",
        "X_test = X_test.reshape(10000, 784) /255.0\n",
        "\n",
        "print(X_train.shape , X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2e-p43TCRRW",
        "outputId": "00e7aa2a-5fd1-446f-ab04-35a963306807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFbxcwcENmQk"
      },
      "source": [
        "# 2.1 Build Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6uqi_S-ZPhn9"
      },
      "outputs": [],
      "source": [
        "def init_params():\n",
        "\n",
        "  #input layer's weigths\n",
        "  W1 = np.random.uniform(-0.5, 0.5, (16,784))\n",
        "  b1 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Hidden layer's weight\n",
        "  W2 = np.random.uniform(-0.5,0.5, (16,16))\n",
        "  b2 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "  # Second Hidden layer' weight\n",
        "  W3 = np.random.uniform(-0.5,0.5 ,(10,16))  # Weights are still to big\n",
        "  b3 = np.zeros((10,1))\n",
        "\n",
        "\n",
        "  return W1,b1,W2,b2,W3,b3\n",
        "\n",
        "\n",
        "\n",
        "W1,b1,W2,b2,W3,b3 = init_params()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRw2jsVWH0I4"
      },
      "source": [
        "## 2.2 Build Activation functions and Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBsAF3drU98m",
        "outputId": "338feec9-0fb4-468a-b7bd-2c24981b3517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.01500404 0.05363104 0.04667131 ... 0.0143118  0.02702216 0.01056243]\n",
            " [0.15155112 0.11413338 0.17878081 ... 0.27793059 0.09523603 0.29578992]\n",
            " [0.04291546 0.1660734  0.1049816  ... 0.05985431 0.10995541 0.06958951]\n",
            " ...\n",
            " [0.00810561 0.05842213 0.06729381 ... 0.02289159 0.0573601  0.02675677]\n",
            " [0.00293193 0.05473835 0.04720537 ... 0.007879   0.05444545 0.01719433]\n",
            " [0.48170369 0.12393148 0.1797272  ... 0.30842978 0.10535148 0.23981536]]\n"
          ]
        }
      ],
      "source": [
        "def ReLU(Z):\n",
        "\n",
        "  return np.maximum(0,Z)\n",
        "\n",
        "\n",
        "def softmax(Z): # CHECK IT !!!\n",
        "\n",
        "  # for one examples is (np.exp(Z) / np.sum( np.exp(Z)))\n",
        "\n",
        "    Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
        "    sum_Z_exp = np.sum(Z_exp, axis=0)\n",
        "    softmax_output = Z_exp / sum_Z_exp\n",
        "\n",
        "    return softmax_output\n",
        "\n",
        "def sigmoid(Z):\n",
        "\n",
        "  return 1 / 1+ np.exp(-Z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def forward_prop(X,W1,b1,W2,b2,W3,b3,dropout = 0.0):\n",
        "  #We are assuming to have 3 layered neural network \n",
        "  #D1 and D2 are mask layers for dropout \n",
        "\n",
        "  D1= None\n",
        "  D2=None\n",
        "\n",
        "  # if there is no dropout layer\n",
        "  if (dropout) == 0.0:\n",
        "    #Forward propagation with 3 layered neural network\n",
        "\n",
        "    #Initialize  Z\n",
        "    #Since X is shape as (6000,784) and our W1 is shape as (16,784) in order to dot product we need to take Transpose of X\n",
        "\n",
        "    Z1 = W1.dot(X.T)+ b1\n",
        "    #Activation with ReLU to given linear regression to feed neural network\n",
        "    A1 = ReLU(Z1)\n",
        "\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "\n",
        "\n",
        "    A2 = ReLU(Z2)\n",
        "\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    #At the end softmax  the output layer to have a probabilistic values\n",
        "    A3 = softmax(Z3)\n",
        "\n",
        " \n",
        "\n",
        "  else: # If there is a droput layer, we are going to build mask for each layer\n",
        "    #Forward propagation with 3 layered neural network \n",
        "\n",
        "    Z1 = W1.dot(X.T)+ b1\n",
        "    #Activation with ReLU to given linear regression to feed neural network\n",
        "    A1 = ReLU(Z1)\n",
        "    '''\n",
        "    This line of code for dropout inspired from Andrew Ng's Deep Learning Specialization Course\n",
        "    https://www.youtube.com/watch?v=D8PJAL-MZv8\n",
        "    '''\n",
        "    #Create a mask for A1 as use probability of dropout from user\n",
        "    D1 = np.random.rand(*A1.shape) > dropout #D1 is mask matrix that check the proability of dropout.\n",
        "                                                          #If the probability is bigger than dropout, it will be 1 otherwise 0\n",
        "\n",
        "    #Apply mask to A1\n",
        "    A1 = A1 * D1\n",
        "    #Normalize A1 to not to change expected value of A1 \n",
        "    A1 = A1 / (1-dropout) # It scales A to not to change expected value of A1 as keeping  probability \n",
        "                          #keeeping probability means 1 - dropout probability\n",
        "\n",
        "   \n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    \n",
        "    #Create a mask for second hidden layer as use probability of dropout from user\n",
        "    D2 = np.random.rand( *A2.shape) > dropout\n",
        "    A2 = A2 * D2\n",
        "    A2 = A2 / (1-dropout)\n",
        "\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    #At the end softmax  the output layer to have a probabilistic values\n",
        "    A3 = softmax(Z3)\n",
        "\n",
        "  return A1,A2,A3,D1,D2\n",
        "\n",
        "A1,A2,A3,D1,D2 = forward_prop(X_train,W1,b1,W2,b2,W3,b3)\n",
        "\n",
        "print(A3)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60000\n"
          ]
        }
      ],
      "source": [
        "print(A1.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFn_mUjHPKab"
      },
      "source": [
        "## 2.3 Back-Propagation with One-hot and derivative of activaton functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5dhSX3T_HAx9"
      },
      "outputs": [],
      "source": [
        "def one_hot(Y): # CHECK IT !!\n",
        "  num_classes = np.max(Y) + 1\n",
        "\n",
        "  one_hot = np.zeros((Y.size,num_classes))\n",
        "  one_hot[np.arange(Y.size), Y] = 1\n",
        "\n",
        "  return one_hot.T\n",
        "\n",
        "def derivative_ReLU(Z):\n",
        "  #Do we need to check between 0-1 ?\n",
        "  return Z > 0\n",
        "\n",
        "#Focus on backpropagation !!! Check how its works ! \n",
        "def backward_prop(X, Y, W1, b1, W2, b2, W3, b3, A1, A2, A3,D1, D2, dropout = 0.0): # CHECK IT !!!!\n",
        "\n",
        "  m = Y.size\n",
        "  Y = one_hot(Y)\n",
        "  #if droput is not used\n",
        "\n",
        "  if (dropout) == 0.0:\n",
        "    \n",
        "    #One hot encoded to see each labels in matrix as 1\n",
        "    \n",
        "\n",
        "    #Derivate of cost function with respect to z3\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "    dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
        "\n",
        "    #Derivate for second hidden layer\n",
        "    dA2 = np.dot(W3.T,dZ3)\n",
        "    dZ2 =  dA2  * derivative_ReLU(A2)\n",
        "    dW2 = (1/ m) * dZ2.dot(A1.T)\n",
        "    dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
        "\n",
        "    #Derivate for first hidden layer\n",
        "\n",
        "    dA1 = np.dot(W2.T,dZ2)\n",
        "    dZ1 = dA1 * derivative_ReLU(A1)\n",
        "    dW1 = (1/m) * dZ1.dot(X)\n",
        "    dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
        "\n",
        "  else: #If droput is used\n",
        "\n",
        "    #Derivate of cost function with respect to z3\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "    dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
        "\n",
        "    #Derivate for second hidden layer\n",
        "    dA2 = np.dot(W3.T,dZ3)\n",
        "    #Dropout mask for second hidden layer\n",
        "    dA2 = dA2 * (D2)\n",
        "    #dA2 = D2 / dropout\n",
        "\n",
        "    dZ2 =  dA2  * derivative_ReLU(A2)\n",
        "    dW2 = (1/ m) * dZ2.dot(A1.T)\n",
        "    dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
        "\n",
        "    #Derivate for first hidden layer\n",
        "\n",
        "    dA1 = np.dot(W2.T,dZ2)\n",
        "    #Dropout mask for first hidden layer\n",
        "    dA1 = dA1 * (D1)\n",
        "    #dA1 = D1 / dropout\n",
        "\n",
        "    dZ1 = dA1 * derivative_ReLU(A1)\n",
        "    dW1 = (1/m) * dZ1.dot(X)\n",
        "    dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
        "    \n",
        "\n",
        "  return dW1,dB1,dW2,dB2,dW3,dB3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tYISPvOQRQn"
      },
      "source": [
        "## 2.4 Update the Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MFbTDfoWQYq-"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "epochs =  200\n",
        "\n",
        "def update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate):\n",
        "\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "\n",
        "    return W1,b1,W2,b2,W3,b3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhb1TpDrQtKt"
      },
      "source": [
        "## 2.5 Try in given epochs time to see the changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "i-QTIprNdkBP"
      },
      "outputs": [],
      "source": [
        "def accuracy_score(A,Y):\n",
        "    size = Y.size\n",
        "    predict = np.argmax(A,0)\n",
        "\n",
        "    correct = np.sum(predict == Y)\n",
        "\n",
        "    accuracy = correct / size\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "#print(predictions.max)\n",
        "#print(y_train)\n",
        "#print(np.sum(y_train == predictions))\n",
        "#print(y_train.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLlbWaJGN7Pw",
        "outputId": "eecb592c-7f38-4726-8423-b3297eaa559c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy in epoch 0 is 0.10631666666666667\n",
            "The accuracy in epoch 100 is 0.4109333333333333\n"
          ]
        }
      ],
      "source": [
        "dropout = 0.1\n",
        "for epoch in range(epochs):\n",
        "   # Forward propagation\n",
        "   A1, A2, A3,D1,D2 = forward_prop(X_train, W1, b1, W2, b2, W3, b3,dropout=dropout)\n",
        "\n",
        "   #Backward propagation\n",
        "   dW1, db1, dW2, db2, dW3, db3 = backward_prop(X_train, y_train, W1, b1, W2, b2, W3, b3, A1, A2, A3,D1,D2,dropout=dropout)\n",
        "\n",
        "   #Updating gradients\n",
        "   W1,b1,W2,b2,W3,b3 = update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "   if epoch % 100 == 0:\n",
        "    acc =accuracy_score(A3,y_train)\n",
        "    print(f'The accuracy in epoch {epoch} is {acc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full Connected Class Version "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hey Greg Please check this part! \n",
        "\n",
        "I havent updated with comments because most of them written in above "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  Missing parts\n",
        "\n",
        "\n",
        "\n",
        "- part d)\n",
        "\n",
        "    1. update rule\n",
        "    2. decay\n",
        "    3. L1 OR L2 regularizator\n",
        "\n",
        "\n",
        "Optimizer is done! \n",
        "\n",
        "May need to check how these gonna change the functions: train, forward and backward prop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "    def __init__(self,input_size,hidden_size,output_size,activation_function,dropout):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.activation_function = activation_function\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.W1 = np.random.uniform(-0.5,0.5,(hidden_size,input_size))\n",
        "        self.b1 = np.zeros((hidden_size,1))\n",
        "\n",
        "        self.W2 = np.random.uniform(-0.5,0.5,(hidden_size,hidden_size))\n",
        "        self.b2 = np.zeros((hidden_size,1))\n",
        "\n",
        "        self.W3 = np.random.uniform(-0.5,0.5,(output_size,hidden_size))\n",
        "        self.b3 = np.zeros((output_size,1))\n",
        "    \n",
        "    #Staticmethod is used to call the function without creating an object\n",
        "    #In this way we can call them in the activation function and deactivation function\n",
        "    @staticmethod  \n",
        "    def ReLU(Z):\n",
        "        return np.maximum(0,Z)\n",
        "    \n",
        "    @staticmethod\n",
        "    def derivative_ReLU(Z):\n",
        "        return Z > 0\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(Z):\n",
        "        # Cap the values of Z within the range [-700, 700], to prevent overflow\n",
        "        Z = np.clip(Z, -700, 700)\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        "    \n",
        "    @staticmethod\n",
        "    def derivative_sigmoid(Z):\n",
        "        return MyNeuralNetwork.sigmoid(Z) * (1 - MyNeuralNetwork.sigmoid(Z))\n",
        "    \n",
        "    @staticmethod\n",
        "    def softmax(Z):\n",
        "        Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
        "        sum_Z_exp = np.sum(Z_exp, axis=0)\n",
        "        softmax_output = Z_exp / sum_Z_exp\n",
        "        return softmax_output\n",
        "    \n",
        "\n",
        "    \n",
        "    def one_hot(self,Y):\n",
        "        num_classes = self.output_size\n",
        "        one_hot = np.zeros((Y.size,num_classes))\n",
        "        one_hot[np.arange(Y.size), Y] = 1\n",
        "        return one_hot.T\n",
        "    \n",
        "    \n",
        "    def activation(self,Z):\n",
        "        if self.activation_function == 'relu':\n",
        "            return self.ReLU(Z)\n",
        "        elif self.activation_function == 'sigmoid':\n",
        "            return self.sigmoid(Z)\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return self.softmax(Z)\n",
        "        else:\n",
        "            raise Exception('Activation function not supported')\n",
        "        \n",
        "    def derivative_activation(self,A):\n",
        "        if self.activation_function == 'relu':\n",
        "            return self.derivative_ReLU(A)\n",
        "        elif self.activation_function == 'sigmoid':\n",
        "            return self.derivative_sigmoid(A)\n",
        "        else:\n",
        "            raise Exception('Activation function not supported')\n",
        "        \n",
        "    def calculate_loss(self,A3,Y):\n",
        "        # Calculate the loss using the cross-entropy loss function\n",
        "        #Calcuation should be based on y_pred and y_true\n",
        "\n",
        "        y_pred = A3\n",
        "        y_true = self.one_hot(Y)\n",
        "\n",
        "        #Clip the y_prediction between epsilon and 1 - epsilon to prevent log(0) error\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "\n",
        "        #Calculate the loss as multi-class cross-entropy loss\n",
        "        loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "        loss = np.sum(loss) / Y.size\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def forward_prop(self,X,dropout):\n",
        "        D1 = None\n",
        "        D2 = None\n",
        "        #Forward propagation with 3 layered neural network\n",
        "        \n",
        "        if dropout == 0.0: # If there is no dropout layer\n",
        "            Z1 = self.W1.dot(X.T) + self.b1\n",
        "            A1 = self.activation(Z1)\n",
        "\n",
        "            Z2 = self.W2.dot(A1) + self.b2\n",
        "            A2 = self.activation(Z2)\n",
        "\n",
        "            Z3 = self.W3.dot(A2) + self.b3\n",
        "            A3 = self.softmax(Z3)\n",
        "        else:              # If there is a dropout layer \n",
        "            \"\"\" This line of code for dropout inspired from Andrew Ng's Deep Learning Specialization Course\"\"\"\n",
        "            \"\"\" https://www.youtube.com/watch?v=D8PJAL-MZv8 \"\"\"\n",
        "\n",
        "            # Create a mask for A1 as use probability of dropout from user\n",
        "            Z1 = self.W1.dot(X.T) + self.b1 \n",
        "            A1 = self.activation(Z1)\n",
        "    \n",
        "            D1 = np.random.rand(*A1.shape) > dropout # D1 is mask matrix that check the proability of dropout.\n",
        "            A1 = A1 * D1                           # If the probability is bigger than dropout, it will be 1 otherwise 0\n",
        "            A1 = A1 / (1 - dropout)                # It scales A to not to change expected value of A1 as keeping  probability\n",
        "\n",
        "            Z2 = self.W2.dot(A1) + self.b2\n",
        "            A2 = self.activation(Z2)\n",
        "\n",
        "            D2 = np.random.rand(*A2.shape) > dropout\n",
        "            A2 = A2 * D2\n",
        "            A2 = A2 / (1 - dropout)\n",
        "\n",
        "            Z3 = self.W3.dot(A2) + self.b3 \n",
        "            A3 = self.softmax(Z3) # At the end softmax  the output layer to have a probabilistic values\n",
        "        \n",
        "        return A1,A2,A3,D1,D2\n",
        "    \n",
        "    def backward_prop(self,X,Y,A1,A2,A3,D1,D2,dropout):\n",
        "        m = Y.size\n",
        "        Y = self.one_hot(Y)\n",
        "        \n",
        "        # Derivate of cost function with respect to z3\n",
        "        dZ3 = A3 - Y\n",
        "        dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "        db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "        # Derivate for second hidden layer\n",
        "        dA2 = np.dot(self.W3.T, dZ3)\n",
        "        if dropout > 0:\n",
        "            dA2 = dA2 * D2  # Apply dropout\n",
        "        dZ2 = dA2 * self.derivative_activation(A2)\n",
        "        dW2 = (1/m) * dZ2.dot(A1.T)\n",
        "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "        # Derivate for first hidden layer\n",
        "        dA1 = np.dot(self.W2.T, dZ2)\n",
        "        if dropout > 0:\n",
        "            dA1 = dA1 * D1  # Apply dropout\n",
        "        dZ1 = dA1 * self.derivative_activation(A1)\n",
        "        dW1 = (1/m) * dZ1.dot(X)\n",
        "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "        # Store the gradients\n",
        "        self.dW1 = dW1\n",
        "        self.db1 = db1\n",
        "        self.dW2 = dW2\n",
        "        self.db2 = db2\n",
        "        self.dW3 = dW3\n",
        "        self.db3 = db3\n",
        "\n",
        "    def update_gradient(self,learning_rate):\n",
        "        self.W1 -= learning_rate * self.dW1\n",
        "        self.b1 -= learning_rate * self.db1\n",
        "        self.W2 -= learning_rate * self.dW2\n",
        "        self.b2 -= learning_rate * self.db2\n",
        "        self.W3 -= learning_rate * self.dW3\n",
        "        self.b3 -= learning_rate * self.db3\n",
        "\n",
        "\n",
        "    def train(self,X,Y,learning_rate,epochs,batch_size): \n",
        "        \"\"\"\n",
        "        Trains the neural network using the given training data.\n",
        "\n",
        "        Parameters:\n",
        "        self : MyNeuralNetwork\n",
        "            The neural network object to train.\n",
        "            \n",
        "\n",
        "        X = numpy.ndarray\n",
        "            The input data, wehere each row is a training example and each column is a feature.\n",
        "\n",
        "        Y = numpy.ndarray\n",
        "            The labels for each training example, where each row is a label. Must have the same number of rows as X.\n",
        "\n",
        "        learning_rate : float\n",
        "            The learning rate to use for weight updates in gradient descent.\n",
        "        epochs : int\n",
        "            The number of times to iterate over the entire training set.\n",
        "\n",
        "        batch_size : int\n",
        "            The number of training examples to split the training set into for mini-batch gradient descent.\n",
        "        \n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        losses = [] # Array to store the loss at each epoch\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "\n",
        "            #Shuffle the dataset at the start of each epoc\n",
        "            permutation = np.random.permutation(m)\n",
        "            X_shuffled = X[permutation]\n",
        "            Y_shuffled = Y[permutation]\n",
        "\n",
        "            #Divide the the dataset into mini-bathces  \n",
        "            for i in range(0,m - (m % batch_size),batch_size):\n",
        "                X_batch = X_shuffled[i:i+batch_size]\n",
        "                Y_batch = Y_shuffled[i:i+batch_size]\n",
        "\n",
        "                # Forward propagation\n",
        "                A1, A2, A3, D1, D2 = self.forward_prop(X_batch, self.dropout)\n",
        "\n",
        "                # Calculate the loss and store it\n",
        "                loss = self.calculate_loss(A3, Y_batch)\n",
        "                epoch_loss += loss\n",
        "\n",
        "\n",
        "                # Backward propagation\n",
        "                self.backward_prop(X_batch, Y_batch, A1, A2, A3, D1, D2, self.dropout)\n",
        "\n",
        "                # Updating gradients\n",
        "                self.update_gradient(learning_rate)\n",
        "\n",
        "            #Calculate the average loss for this epoch and store it \n",
        "            average_epoch_loss = epoch_loss / (m // batch_size)\n",
        "            losses.append(average_epoch_loss)\n",
        "\n",
        "            #Print the loss at each epoch with decimal point 5\n",
        "            print(f'The loss in epoch {epoch} is : {average_epoch_loss:.5f}')\n",
        "        return losses\n",
        "            \n",
        "\n",
        "    def predict(self,X):\n",
        "        A1, A2, A3, D1, D2 = self.forward_prop(X, dropout=0.0)\n",
        "        return np.argmax(A3, axis=0)\n",
        "\n",
        "    def accuracy_score(self,X,Y):\n",
        "        size = Y.size\n",
        "        predict = self.predict(X)\n",
        "        correct = np.sum(predict == Y)\n",
        "        accuracy = correct / size\n",
        "        return accuracy\n",
        "\n",
        "    def test(self,X,Y):\n",
        "        accuracy = self.accuracy_score(X,Y)\n",
        "        print(f'The accuracy is: {accuracy*100}')\n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The loss in epoch 0 is : 1.06053\n",
            "The loss in epoch 1 is : 0.63389\n",
            "The loss in epoch 2 is : 0.52607\n",
            "The loss in epoch 3 is : 0.46748\n",
            "The loss in epoch 4 is : 0.42829\n",
            "The loss in epoch 5 is : 0.40069\n",
            "The loss in epoch 6 is : 0.37589\n",
            "The loss in epoch 7 is : 0.35657\n",
            "The loss in epoch 8 is : 0.33846\n",
            "The loss in epoch 9 is : 0.32522\n",
            "The loss in epoch 10 is : 0.31306\n",
            "The loss in epoch 11 is : 0.30304\n",
            "The loss in epoch 12 is : 0.29481\n",
            "The loss in epoch 13 is : 0.28919\n",
            "The loss in epoch 14 is : 0.28215\n",
            "The loss in epoch 15 is : 0.27888\n",
            "The loss in epoch 16 is : 0.26884\n",
            "The loss in epoch 17 is : 0.26656\n",
            "The loss in epoch 18 is : 0.25516\n",
            "The loss in epoch 19 is : 0.25544\n",
            "The loss in epoch 20 is : 0.25428\n",
            "The loss in epoch 21 is : 0.24973\n",
            "The loss in epoch 22 is : 0.24339\n",
            "The loss in epoch 23 is : 0.23420\n",
            "The loss in epoch 24 is : 0.23950\n",
            "The loss in epoch 25 is : 0.23308\n",
            "The loss in epoch 26 is : 0.23108\n",
            "The loss in epoch 27 is : 0.22395\n",
            "The loss in epoch 28 is : 0.22501\n",
            "The loss in epoch 29 is : 0.22446\n",
            "The loss in epoch 30 is : 0.22043\n",
            "The loss in epoch 31 is : 0.21200\n",
            "The loss in epoch 32 is : 0.21588\n",
            "The loss in epoch 33 is : 0.21455\n",
            "The loss in epoch 34 is : 0.20898\n",
            "The loss in epoch 35 is : 0.20239\n",
            "The loss in epoch 36 is : 0.20785\n",
            "The loss in epoch 37 is : 0.20715\n",
            "The loss in epoch 38 is : 0.20147\n",
            "The loss in epoch 39 is : 0.19743\n",
            "The loss in epoch 40 is : 0.19358\n",
            "The loss in epoch 41 is : 0.19705\n",
            "The loss in epoch 42 is : 0.19839\n",
            "The loss in epoch 43 is : 0.19797\n",
            "The loss in epoch 44 is : 0.19364\n",
            "The loss in epoch 45 is : 0.18977\n",
            "The loss in epoch 46 is : 0.18594\n",
            "The loss in epoch 47 is : 0.19090\n",
            "The loss in epoch 48 is : 0.18382\n",
            "The loss in epoch 49 is : 0.18790\n"
          ]
        }
      ],
      "source": [
        "# Create a neural network object\n",
        "nn = MyNeuralNetwork(input_size=784, hidden_size=35, output_size=10, activation_function='relu',dropout=0.1)\n",
        "\n",
        "# Train the neural network\n",
        "losses = nn.train(X_train, y_train, learning_rate=0.1, epochs=50, batch_size=36)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy is: 96.76\n"
          ]
        }
      ],
      "source": [
        "# Test the neural network\n",
        "nn.test(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Plot the loss curve\n",
        "\n",
        "\n",
        "plt.plot(losses)\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNLCAXIYYGmpU8xJwnPWCXF",
      "collapsed_sections": [
        "_WGR6imnPESL",
        "aFbxcwcENmQk",
        "IRw2jsVWH0I4"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

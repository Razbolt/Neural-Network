{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Razbolt/Neural-Network/blob/main/Neural_Network_Math_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGR6imnPESL"
      },
      "source": [
        "# 1.1 Install the MNIST and requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j7joMCD_8U_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import load_digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9rb-9S-KJrD",
        "outputId": "67a98d93-63de-44df-afcf-2b20f8dfb497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-mnist\n",
            "  Using cached python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement quiet (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for quiet\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install python-mnist -- quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qc1NylZkLXAz"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "# Using keras libraries to take MNIST dataset\n",
        "(X_train, y_train,), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6uJCuvDsoP2"
      },
      "source": [
        "## 1.1 Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4kGn7-Ps25y"
      },
      "source": [
        "Before procesing, we need to check the MNIST Data.\n",
        "\n",
        "As we can see the X_train data is numbered from 0 to 255. In order to use them we need to normalize them as dividing to 255 each of the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ1--VdWCEzT",
        "outputId": "69f3a8c0-361f-42e4-ea3b-07d7a0d456a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UIxpHoosaud"
      },
      "source": [
        "We also need to check the shapes before feeding them into model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJs76_reOzPj",
        "outputId": "58d2aaa7-8d23-4874-ebe1-d9db9bec039c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zdUnLliLXu2",
        "outputId": "a875fade-abc2-4672-e997-f31a22e726a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9u7khFALZFa",
        "outputId": "c548320b-5844-426d-acd8-87af98bae5e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fHFqMAQBRrf",
        "outputId": "e16a193b-4b11-4f45-b77b-404151b3117e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784) (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Since we see the shapes are (6000,28,28) we need to reshape them to feed our model\n",
        "\n",
        "X_train = X_train.reshape(60000, 784) / 255.0\n",
        "\n",
        "X_test = X_test.reshape(10000, 784) /255.0\n",
        "\n",
        "print(X_train.shape , X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2e-p43TCRRW",
        "outputId": "00e7aa2a-5fd1-446f-ab04-35a963306807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFbxcwcENmQk"
      },
      "source": [
        "# 2.1 Build Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6uqi_S-ZPhn9"
      },
      "outputs": [],
      "source": [
        "def init_params():\n",
        "\n",
        "  #input layer's weigths\n",
        "  W1 = np.random.uniform(-0.5, 0.5, (16,784))\n",
        "  b1 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Hidden layer's weight\n",
        "  W2 = np.random.uniform(-0.5,0.5, (16,16))\n",
        "  b2 = np.zeros((16,1))\n",
        "\n",
        "\n",
        "  # Second Hidden layer' weight\n",
        "  W3 = np.random.uniform(-0.5,0.5 ,(10,16))  # Weights are still to big\n",
        "  b3 = np.zeros((10,1))\n",
        "\n",
        "\n",
        "  return W1,b1,W2,b2,W3,b3\n",
        "\n",
        "\n",
        "\n",
        "W1,b1,W2,b2,W3,b3 = init_params()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRw2jsVWH0I4"
      },
      "source": [
        "## 2.2 Build Activation functions and Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBsAF3drU98m",
        "outputId": "338feec9-0fb4-468a-b7bd-2c24981b3517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.01500404 0.05363104 0.04667131 ... 0.0143118  0.02702216 0.01056243]\n",
            " [0.15155112 0.11413338 0.17878081 ... 0.27793059 0.09523603 0.29578992]\n",
            " [0.04291546 0.1660734  0.1049816  ... 0.05985431 0.10995541 0.06958951]\n",
            " ...\n",
            " [0.00810561 0.05842213 0.06729381 ... 0.02289159 0.0573601  0.02675677]\n",
            " [0.00293193 0.05473835 0.04720537 ... 0.007879   0.05444545 0.01719433]\n",
            " [0.48170369 0.12393148 0.1797272  ... 0.30842978 0.10535148 0.23981536]]\n"
          ]
        }
      ],
      "source": [
        "def ReLU(Z):\n",
        "\n",
        "  return np.maximum(0,Z)\n",
        "\n",
        "\n",
        "def softmax(Z): # CHECK IT !!!\n",
        "\n",
        "  # for one examples is (np.exp(Z) / np.sum( np.exp(Z)))\n",
        "\n",
        "    Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
        "    sum_Z_exp = np.sum(Z_exp, axis=0)\n",
        "    softmax_output = Z_exp / sum_Z_exp\n",
        "\n",
        "    return softmax_output\n",
        "\n",
        "def sigmoid(Z):\n",
        "\n",
        "  return 1 / 1+ np.exp(-Z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def forward_prop(X,W1,b1,W2,b2,W3,b3,dropout = 0.0):\n",
        "  #We are assuming to have 3 layered neural network \n",
        "  #D1 and D2 are mask layers for dropout \n",
        "\n",
        "  D1= None\n",
        "  D2=None\n",
        "\n",
        "  # if there is no dropout layer\n",
        "  if (dropout) == 0.0:\n",
        "    #Forward propagation with 3 layered neural network\n",
        "\n",
        "    #Initialize  Z\n",
        "    #Since X is shape as (6000,784) and our W1 is shape as (16,784) in order to dot product we need to take Transpose of X\n",
        "\n",
        "    Z1 = W1.dot(X.T)+ b1\n",
        "    #Activation with ReLU to given linear regression to feed neural network\n",
        "    A1 = ReLU(Z1)\n",
        "\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "\n",
        "\n",
        "    A2 = ReLU(Z2)\n",
        "\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    #At the end softmax  the output layer to have a probabilistic values\n",
        "    A3 = softmax(Z3)\n",
        "\n",
        " \n",
        "\n",
        "  else: # If there is a droput layer, we are going to build mask for each layer\n",
        "    #Forward propagation with 3 layered neural network \n",
        "\n",
        "    Z1 = W1.dot(X.T)+ b1\n",
        "    #Activation with ReLU to given linear regression to feed neural network\n",
        "    A1 = ReLU(Z1)\n",
        "    \n",
        "    #Create a mask for A1 as use probability of dropout from user\n",
        "    D1 = np.random.rand(*A1.shape) > dropout #D1 is mask matrix that check the proability of dropout.\n",
        "                                                          #If the probability is bigger than dropout, it will be 1 otherwise 0\n",
        "\n",
        "    #Apply mask to A1\n",
        "    A1 = A1 * D1\n",
        "    #Normalize A1 to not to change expected value of A1 \n",
        "    A1 = A1 / (1-dropout) # It scales A to not to change expected value of A1 as keeping  probability \n",
        "                          #keeeping probability means 1 - dropout probability\n",
        "\n",
        "   \n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = ReLU(Z2)\n",
        "    \n",
        "    #Create a mask for second hidden layer as use probability of dropout from user\n",
        "    D2 = np.random.rand( *A2.shape) > dropout\n",
        "    A2 = A2 * D2\n",
        "    A2 = A2 / (1-dropout)\n",
        "\n",
        "    Z3 = W3.dot(A2) + b3\n",
        "    #At the end softmax  the output layer to have a probabilistic values\n",
        "    A3 = softmax(Z3)\n",
        "\n",
        "  return A1,A2,A3,D1,D2\n",
        "\n",
        "A1,A2,A3,D1,D2 = forward_prop(X_train,W1,b1,W2,b2,W3,b3)\n",
        "\n",
        "print(A3)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60000\n"
          ]
        }
      ],
      "source": [
        "print(A1.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFn_mUjHPKab"
      },
      "source": [
        "## 2.3 Back-Propagation with One-hot and derivative of activaton functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5dhSX3T_HAx9"
      },
      "outputs": [],
      "source": [
        "def one_hot(Y): # CHECK IT !!\n",
        "  num_classes = np.max(Y) + 1\n",
        "\n",
        "  one_hot = np.zeros((Y.size,num_classes))\n",
        "  one_hot[np.arange(Y.size), Y] = 1\n",
        "\n",
        "  return one_hot.T\n",
        "\n",
        "def derivative_ReLU(Z):\n",
        "  #Do we need to check between 0-1 ?\n",
        "  return Z > 0\n",
        "\n",
        "#Focus on backpropagation !!! Check how its works ! \n",
        "def backward_prop(X, Y, W1, b1, W2, b2, W3, b3, A1, A2, A3,D1, D2, dropout = 0.0): # CHECK IT !!!!\n",
        "\n",
        "  m = Y.size\n",
        "  Y = one_hot(Y)\n",
        "  #if droput is not used\n",
        "\n",
        "  if (dropout) == 0.0:\n",
        "    \n",
        "    #One hot encoded to see each labels in matrix as 1\n",
        "    \n",
        "\n",
        "    #Derivate of cost function with respect to z3\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "    dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
        "\n",
        "    #Derivate for second hidden layer\n",
        "    dA2 = np.dot(W3.T,dZ3)\n",
        "    dZ2 =  dA2  * derivative_ReLU(A2)\n",
        "    dW2 = (1/ m) * dZ2.dot(A1.T)\n",
        "    dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
        "\n",
        "    #Derivate for first hidden layer\n",
        "\n",
        "    dA1 = np.dot(W2.T,dZ2)\n",
        "    dZ1 = dA1 * derivative_ReLU(A1)\n",
        "    dW1 = (1/m) * dZ1.dot(X)\n",
        "    dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
        "\n",
        "  else: #If droput is used\n",
        "\n",
        "    #Derivate of cost function with respect to z3\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = (1/m) * dZ3.dot(A2.T)\n",
        "    dB3 = (1/m) * np.sum(dZ3,axis = 1,keepdims=True)\n",
        "\n",
        "    #Derivate for second hidden layer\n",
        "    dA2 = np.dot(W3.T,dZ3)\n",
        "    #Dropout mask for second hidden layer\n",
        "    dA2 = dA2 * (D2)\n",
        "    #dA2 = D2 / dropout\n",
        "\n",
        "    dZ2 =  dA2  * derivative_ReLU(A2)\n",
        "    dW2 = (1/ m) * dZ2.dot(A1.T)\n",
        "    dB2 = (1/m) * np.sum(dZ2,axis =1,keepdims=True)\n",
        "\n",
        "    #Derivate for first hidden layer\n",
        "\n",
        "    dA1 = np.dot(W2.T,dZ2)\n",
        "    #Dropout mask for first hidden layer\n",
        "    dA1 = dA1 * (D1)\n",
        "    #dA1 = D1 / dropout\n",
        "\n",
        "    dZ1 = dA1 * derivative_ReLU(A1)\n",
        "    dW1 = (1/m) * dZ1.dot(X)\n",
        "    dB1 =(1/m) * np.sum(dZ1, axis = 1,keepdims=True)\n",
        "    \n",
        "\n",
        "  return dW1,dB1,dW2,dB2,dW3,dB3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tYISPvOQRQn"
      },
      "source": [
        "## 2.4 Update the Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MFbTDfoWQYq-"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "epochs =  200\n",
        "\n",
        "def update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate):\n",
        "\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "\n",
        "    return W1,b1,W2,b2,W3,b3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhb1TpDrQtKt"
      },
      "source": [
        "## 2.5 Try in given epochs time to see the changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "i-QTIprNdkBP"
      },
      "outputs": [],
      "source": [
        "def accuracy_score(A,Y):\n",
        "    size = Y.size\n",
        "    predict = np.argmax(A,0)\n",
        "\n",
        "    correct = np.sum(predict == Y)\n",
        "\n",
        "    accuracy = correct / size\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "#print(predictions.max)\n",
        "#print(y_train)\n",
        "#print(np.sum(y_train == predictions))\n",
        "#print(y_train.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLlbWaJGN7Pw",
        "outputId": "eecb592c-7f38-4726-8423-b3297eaa559c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy in epoch 0 is 0.10631666666666667\n",
            "The accuracy in epoch 100 is 0.4109333333333333\n"
          ]
        }
      ],
      "source": [
        "dropout = 0.1\n",
        "for epoch in range(epochs):\n",
        "   # Forward propagation\n",
        "   A1, A2, A3,D1,D2 = forward_prop(X_train, W1, b1, W2, b2, W3, b3,dropout=dropout)\n",
        "\n",
        "   #Backward propagation\n",
        "   dW1, db1, dW2, db2, dW3, db3 = backward_prop(X_train, y_train, W1, b1, W2, b2, W3, b3, A1, A2, A3,D1,D2,dropout=dropout)\n",
        "\n",
        "   #Updating gradients\n",
        "   W1,b1,W2,b2,W3,b3 = update_gradient(dW1,db1,dW2,db2,dW3,db3,W1,b1,W2,b2,W3,b3,learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "   if epoch % 100 == 0:\n",
        "    acc =accuracy_score(A3,y_train)\n",
        "    print(f'The accuracy in epoch {epoch} is {acc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full Connected Class Version "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  Missing parts\n",
        "\n",
        "Only update rule\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "    def __init__(self,input_size,hidden_sizes,output_size,activation_function,dropout):\n",
        "        self.layer_sizes = [input_size] + hidden_sizes + [output_size] # Number of neurons in each layer\n",
        "        self.activation_function = activation_function                 # Activation function to use\n",
        "        self.dropout = dropout                                         # Dropout probability\n",
        "        self.weights = []                                              # Weights for each layer\n",
        "        self.biases = []                                               # Biases for each layer\n",
        "        self.input_size = input_size                                   # Input size\n",
        "        self.output_size = output_size                                 # Output size              \n",
        "        self.hidden_sizes = hidden_sizes                               # Hidden layer sizes\n",
        "\n",
        "\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "            self.weights.append(np.random.uniform(-0.5, 0.5, (self.layer_sizes[i+1], self.layer_sizes[i])))\n",
        "            self.biases.append(np.zeros((self.layer_sizes[i+1], 1)))\n",
        "\n",
        "            #Debug: Print shapes\n",
        "            print(f\"Layers in initilization: {i}, Weights shape: {self.weights[i].shape}, Biases shape: {self.biases[i].shape}\")\n",
        "    \n",
        "    #Staticmethod is used to call the function without creating an object\n",
        "    #In this way we can call them in the activation function and deactivation function\n",
        "    @staticmethod  \n",
        "    def ReLU(Z):\n",
        "        return np.maximum(0,Z)\n",
        "    \n",
        "    @staticmethod\n",
        "    def derivative_ReLU(Z):\n",
        "        return Z > 0\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(Z):\n",
        "        # Cap the values of Z within the range [-700, 700], to prevent overflow\n",
        "        Z = np.clip(Z, -700, 700)\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        "    \n",
        "    @staticmethod\n",
        "    def derivative_sigmoid(Z):\n",
        "        return MyNeuralNetwork.sigmoid(Z) * (1 - MyNeuralNetwork.sigmoid(Z))\n",
        "    \n",
        "    @staticmethod\n",
        "    def softmax(Z):\n",
        "        Z_exp = np.exp(Z - np.max(Z, axis=0))\n",
        "        sum_Z_exp = np.sum(Z_exp, axis=0)\n",
        "        softmax_output = Z_exp / sum_Z_exp\n",
        "        return softmax_output\n",
        "    \n",
        "\n",
        "    \n",
        "    def one_hot(self,Y):\n",
        "        num_classes = self.output_size\n",
        "        one_hot = np.zeros((Y.size,num_classes))\n",
        "        one_hot[np.arange(Y.size), Y] = 1\n",
        "        return one_hot.T\n",
        "    \n",
        "    \n",
        "    def activation(self,Z):\n",
        "        if self.activation_function == 'relu':\n",
        "            return self.ReLU(Z)\n",
        "        elif self.activation_function == 'sigmoid':\n",
        "            return self.sigmoid(Z)\n",
        "        elif self.activation_function == 'softmax':\n",
        "            return self.softmax(Z)\n",
        "        else:\n",
        "            raise Exception('Activation function not supported')\n",
        "        \n",
        "    def derivative_activation(self,A):\n",
        "        if self.activation_function == 'relu':\n",
        "            return self.derivative_ReLU(A)\n",
        "        elif self.activation_function == 'sigmoid':\n",
        "            return self.derivative_sigmoid(A)\n",
        "        else:\n",
        "            raise Exception('Activation function not supported')\n",
        "        \n",
        "    def calculate_loss(self,A3,Y):\n",
        "        # Calculate the loss using the cross-entropy loss function\n",
        "        #Calcuation should be based on y_pred and y_true\n",
        "\n",
        "        y_pred = A3\n",
        "        y_true = self.one_hot(Y)\n",
        "\n",
        "        #Clip the y_prediction between epsilon and 1 - epsilon to prevent log(0) error\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "\n",
        "        #Calculate the loss as multi-class cross-entropy loss\n",
        "        loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "        loss = np.sum(loss) / Y.size\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    \n",
        "    def forward_prop(self,X):\n",
        "        # Forward propagation\n",
        "        A = X.T\n",
        "        layer_outputs = {}\n",
        "        D = {} # Dropout mask matrices\n",
        "\n",
        "        #Loop over each layer in the network\n",
        "        for layer in range(len(self.layer_sizes)-1):\n",
        "            # Compute the linear output Z for the current layer\n",
        "            Z = self.weights[layer].dot(A) + self.biases[layer]\n",
        "\n",
        "            #Check for the if not the last layer, and apply the activation function\n",
        "            if (layer != len(self.layer_sizes) - 2 ):\n",
        "                A = self.activation(Z)\n",
        "                #Check for dropout and apply it accordingly \n",
        "                if self.dropout > 0: #apply dropout\n",
        "\n",
        "                    '''\n",
        "\n",
        "                    This line of code for dropout inspired from Andrew Ng's Deep Learning Specialization Course\n",
        "                    https://www.youtube.com/watch?v=D8PJAL-MZv8\n",
        "                    '''       \n",
        "\n",
        "                    #Create a dropout mask matrix to be dropoutted neurons\n",
        "                    D[layer] = np.random.rand(*A.shape) > self.dropout\n",
        "                    #Apply the mask to A\n",
        "                    A = A * D[layer]\n",
        "                    #Normalize A to not to change expected value of A as keeping  probability\n",
        "                    A = A / (1 - self.dropout)\n",
        "                    \n",
        "            #if there is no dropout\n",
        "            else: #for the last layer use softmax activation function\n",
        "                A = self.softmax(Z)\n",
        "            layer_outputs[layer] = (A,Z)\n",
        "            #Debuging: Print shapes\n",
        "            #print(f\"Forward Propagation, Layer: {layer}, A shape: {A.shape}, Z shape: {Z.shape}\")\n",
        "        \n",
        "        return layer_outputs,D\n",
        "    \n",
        "    def backward_prop(self, X, Y, layer_outputs, D):\n",
        "\n",
        "        m = X.shape[0]  # Number of training examples\n",
        "        Y = self.one_hot(Y)  # One hot encode the labels\n",
        "        gradients = {}  # Dictionary to store the gradients for each layer\n",
        "        dZ = None \n",
        "        for layer in reversed(range(len(self.layer_sizes) - 1)): # Layers are backwardley calculate the gradients\n",
        "            A, Z = layer_outputs[layer]                          # Get the layer's activations and linear outputs\n",
        "\n",
        "            if layer == len(self.layer_sizes) - 2:  # Gradient for the last layer\n",
        "                dZ = A - Y\n",
        "                \n",
        "            else:                                  # Gradient for the hidden layers\n",
        "                dA = np.dot(self.weights[layer + 1].T, dZ)\n",
        "                if self.dropout > 0:               # Apply dropout   \n",
        "                    dA *= D[layer]\n",
        "                dZ = dA * self.derivative_activation(Z)\n",
        "\n",
        "            prev_A = layer_outputs[layer - 1][0] if layer != 0 else X.T     #If layer is not last one takes the previous layer's activation\n",
        "                                                                            #If layers is last one takes the input X\n",
        "            # Debugging: Print shapes\n",
        "            #print(f\" Backward_prop ,Layer: {layer}, prev_A shape: {prev_A.shape}, dZ.T shape: {dZ.T.shape}\")\n",
        "\n",
        "            #Calculate the gradients for each layer  and store them in gradients dictionary\n",
        "            dW = np.dot(dZ, prev_A.T) / m\n",
        "            db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "\n",
        "            gradients[layer] = (dW, db)\n",
        "\n",
        "        return gradients\n",
        "\n",
        "\n",
        "        \n",
        "    \n",
        "    def update_gradient(self,gradients,learning_rate):\n",
        "        # Update the weights and biases using gradient descent\n",
        "        for layer in range(len(self.layer_sizes) - 1):  # Corrected here\n",
        "            dW,db = gradients[layer]\n",
        "\n",
        "            self.weights[layer] -= learning_rate * dW\n",
        "            self.biases[layer] -= learning_rate * db\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    def train(self,X,Y,learning_rate,epochs,batch_size,decay_rate,decay_step): \n",
        "        \"\"\"\n",
        "        Trains the neural network using the given training data.\n",
        "\n",
        "        Parameters:\n",
        "        self : MyNeuralNetwork\n",
        "            The neural network object to train.\n",
        "            \n",
        "\n",
        "        X = numpy.ndarray\n",
        "            The input data, wehere each row is a training example and each column is a feature.\n",
        "\n",
        "        Y = numpy.ndarray\n",
        "            The labels for each training example, where each row is a label. Must have the same number of rows as X.\n",
        "\n",
        "        learning_rate : float\n",
        "            The learning rate to use for weight updates in gradient descent.\n",
        "        epochs : int\n",
        "            The number of times to iterate over the entire training set.\n",
        "\n",
        "        batch_size : int\n",
        "            The number of training examples to split the training set into for mini-batch gradient descent.\n",
        "        \n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        losses = [] # Array to store the loss at each epoch\n",
        "        accuracies = [] # Array to store the accuracy at each epoch\n",
        "        learning_rate = learning_rate\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            epoch_correct = 0\n",
        "\n",
        "            #Applying the learning rate decay at specific steps\n",
        "            if(epoch % decay_step == 0 and epochs != 0):\n",
        "                learning_rate = learning_rate * (1.0/ (1.0 +decay_rate * epoch))\n",
        "\n",
        "                #Print the new learning rate at each decay step\n",
        "                print(f'The learning rate at epoch {epoch} is :{learning_rate}')\n",
        "\n",
        "            #Shuffle the dataset at the start of each epoc\n",
        "            permutation = np.random.permutation(m)\n",
        "            X_shuffled = X[permutation]\n",
        "            Y_shuffled = Y[permutation]\n",
        "\n",
        "            #Divide the the dataset into mini-bathces  \n",
        "            for i in range(0,m - (m % batch_size),batch_size):\n",
        "                X_batch = X_shuffled[i:i+batch_size]\n",
        "                Y_batch = Y_shuffled[i:i+batch_size]\n",
        "\n",
        "                # Forward propagation\n",
        "                layer_outpus,D = self.forward_prop(X_batch )\n",
        "\n",
        "                # Calculate the loss and store it\n",
        "                last_layer = len(self.layer_sizes) - 2\n",
        "                loss = self.calculate_loss(layer_outpus[last_layer][0], Y_batch)\n",
        "                epoch_loss += loss\n",
        "\n",
        "\n",
        "                # Calculate the number of correct predictions\n",
        "                predictions = np.argmax(layer_outpus[last_layer][0], axis=0)\n",
        "                actuals = (Y_batch)\n",
        "                epoch_correct += np.sum(predictions == actuals)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "                # Backward propagation\n",
        "                gradients=self.backward_prop(X_batch, Y_batch, layer_outpus,D)\n",
        "\n",
        "                # Updating gradients\n",
        "                self.update_gradient(gradients,learning_rate)\n",
        "\n",
        "            #Calculate the average loss for this epoch and store it \n",
        "            average_epoch_loss = epoch_loss / (m // batch_size)\n",
        "            average_epoch_accuracy = epoch_correct / (m)\n",
        "            accuracies.append(average_epoch_accuracy)\n",
        "            losses.append(average_epoch_loss)\n",
        "\n",
        "            #Print the loss at each epoch with decimal point 5\n",
        "            print(f'Ine epoch  {epoch} loss  is : {average_epoch_loss:.5f} and accuracy is: {average_epoch_accuracy:.5f}')\n",
        "            \n",
        "        return losses,accuracies\n",
        "            \n",
        "\n",
        "    def predict(self,X):\n",
        "        layer_outputs,_ = self.forward_prop(X)\n",
        "        last_layer_acitvation = layer_outputs[len(self.layer_sizes) - 2][0]\n",
        "        predictions = np.argmax(last_layer_acitvation, axis=0)\n",
        "        return (last_layer_acitvation,predictions)\n",
        "\n",
        "    def accuracy_score(self,X,Y):\n",
        "        #The last layer of A3 is the probabilistic values of each class to calculate loss \n",
        "        last_layer_activation,predictions = self.predict(X)\n",
        "\n",
        "        size = Y.size\n",
        "        \n",
        "        correct = np.sum(predictions == Y)\n",
        "        accuracy = correct / size\n",
        "        return last_layer_activation,accuracy\n",
        "\n",
        "    def test(self,X,Y):\n",
        "        last_layer_acitvation,accuracy = self.accuracy_score(X,Y)\n",
        "        test_loss = self.calculate_loss(last_layer_acitvation, Y)\n",
        "        \n",
        "        return test_loss,accuracy\n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation for different Architectures "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets create different Architecutres to comparsion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. First Neural Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this Neural Network\n",
        "- Its builded as 3 hidden layer as their inputs sizes 16 consequently\n",
        "- Activation function as 'Sigmoid' \n",
        "- No dropout applied \n",
        "- Mini batch size as 56 \n",
        "- Learning rate is initilized as 1 \n",
        "- decay rate is 0.1 to decrease learning rate in steps in 10 (decay_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layers in initilization: 0, Weights shape: (16, 784), Biases shape: (16, 1)\n",
            "Layers in initilization: 1, Weights shape: (16, 16), Biases shape: (16, 1)\n",
            "Layers in initilization: 2, Weights shape: (10, 16), Biases shape: (10, 1)\n"
          ]
        }
      ],
      "source": [
        "# Create a neural network object\n",
        "nn = MyNeuralNetwork(input_size=784, hidden_sizes=[16,16], output_size=10, activation_function='sigmoid',dropout=0.0)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The learning rate at epoch 0 is :1.0\n",
            "In 0 loss  is : 0.90597 and accuracy is: 0.06548\n",
            "In 1 loss  is : 0.43781 and accuracy is: 0.06400\n",
            "In 2 loss  is : 0.37345 and accuracy is: 0.06328\n",
            "In 3 loss  is : 0.33684 and accuracy is: 0.06415\n",
            "In 4 loss  is : 0.31808 and accuracy is: 0.06140\n",
            "In 5 loss  is : 0.29855 and accuracy is: 0.06478\n",
            "In 6 loss  is : 0.28824 and accuracy is: 0.06673\n",
            "In 7 loss  is : 0.27730 and accuracy is: 0.06752\n",
            "In 8 loss  is : 0.26471 and accuracy is: 0.06532\n",
            "In 9 loss  is : 0.26167 and accuracy is: 0.06483\n",
            "The learning rate at epoch 10 is :0.5\n",
            "In 10 loss  is : 0.22422 and accuracy is: 0.06447\n",
            "In 11 loss  is : 0.21581 and accuracy is: 0.06577\n",
            "In 12 loss  is : 0.21283 and accuracy is: 0.06542\n",
            "In 13 loss  is : 0.20977 and accuracy is: 0.06298\n",
            "In 14 loss  is : 0.20842 and accuracy is: 0.06423\n",
            "In 15 loss  is : 0.20404 and accuracy is: 0.06265\n",
            "In 16 loss  is : 0.20148 and accuracy is: 0.06352\n",
            "In 17 loss  is : 0.19985 and accuracy is: 0.06187\n",
            "In 18 loss  is : 0.19523 and accuracy is: 0.06652\n",
            "In 19 loss  is : 0.19484 and accuracy is: 0.06785\n",
            "The learning rate at epoch 20 is :0.16666666666666666\n",
            "In 20 loss  is : 0.17304 and accuracy is: 0.06488\n",
            "In 21 loss  is : 0.16961 and accuracy is: 0.06780\n",
            "In 22 loss  is : 0.16955 and accuracy is: 0.06432\n",
            "In 23 loss  is : 0.16801 and accuracy is: 0.06473\n",
            "In 24 loss  is : 0.16728 and accuracy is: 0.06478\n",
            "In 25 loss  is : 0.16626 and accuracy is: 0.06263\n",
            "In 26 loss  is : 0.16459 and accuracy is: 0.06428\n",
            "In 27 loss  is : 0.16395 and accuracy is: 0.06492\n",
            "In 28 loss  is : 0.16325 and accuracy is: 0.06117\n",
            "In 29 loss  is : 0.16188 and accuracy is: 0.06478\n",
            "The learning rate at epoch 30 is :0.041666666666666664\n",
            "In 30 loss  is : 0.15511 and accuracy is: 0.06465\n",
            "In 31 loss  is : 0.15432 and accuracy is: 0.06265\n",
            "In 32 loss  is : 0.15378 and accuracy is: 0.06430\n",
            "In 33 loss  is : 0.15362 and accuracy is: 0.06677\n",
            "In 34 loss  is : 0.15349 and accuracy is: 0.06385\n",
            "In 35 loss  is : 0.15321 and accuracy is: 0.06455\n",
            "In 36 loss  is : 0.15270 and accuracy is: 0.06548\n",
            "In 37 loss  is : 0.15267 and accuracy is: 0.06345\n",
            "In 38 loss  is : 0.15214 and accuracy is: 0.06230\n",
            "In 39 loss  is : 0.15208 and accuracy is: 0.06578\n",
            "The learning rate at epoch 40 is :0.008333333333333333\n",
            "In 40 loss  is : 0.15014 and accuracy is: 0.06693\n",
            "In 41 loss  is : 0.14991 and accuracy is: 0.06307\n",
            "In 42 loss  is : 0.14990 and accuracy is: 0.06653\n",
            "In 43 loss  is : 0.14984 and accuracy is: 0.06615\n",
            "In 44 loss  is : 0.14979 and accuracy is: 0.06438\n",
            "In 45 loss  is : 0.14970 and accuracy is: 0.06763\n",
            "In 46 loss  is : 0.14965 and accuracy is: 0.06557\n",
            "In 47 loss  is : 0.14964 and accuracy is: 0.06545\n",
            "In 48 loss  is : 0.14959 and accuracy is: 0.06492\n",
            "In 49 loss  is : 0.14935 and accuracy is: 0.06500\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the neural network\n",
        "losses,accuracies = nn.train(X_train, y_train, learning_rate=1, epochs=50, batch_size=56,decay_rate=0.1,decay_step=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.30272844221018597, Test Accuracy: 0.9503\n"
          ]
        }
      ],
      "source": [
        "# Test the neural network\n",
        "loss,acc = nn.test(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {acc}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Plot the loss curve\n",
        "\n",
        "\n",
        "plt.plot(losses)\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNLCAXIYYGmpU8xJwnPWCXF",
      "collapsed_sections": [
        "_WGR6imnPESL",
        "aFbxcwcENmQk",
        "IRw2jsVWH0I4"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
